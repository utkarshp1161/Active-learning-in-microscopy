{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2axl6kznB5_"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/utkarshp1161/Active-learning-in-microscopy/blob/main/notebooks/multi_objective_BO_SVDKL.ipynb)\n",
    "\n",
    "# Multi-objective Active learning using DigitalTwin microscope: Stochastic Variational Deep kernel learning in Gpytorch and BO loop in Botorch. [Recommended to take a GPU instance]\n",
    "- For single objective please [see](https://github.com/utkarshp1161/Active-learning-in-microscopy/blob/main/notebooks/single_objective_BO_SVDKL.ipynb) \n",
    "\n",
    "Prepared by [Utkarsh Pratiush](https://github.com/utkarshp1161)\n",
    "- Get in touch if any doubts or discussion [utkarshp1161@gmail.com]\n",
    "- For reading related to this notebook please refer to [wilson et al 2016](https://arxiv.org/abs/1611.00336), [wilson et al 2015](https://arxiv.org/abs/1511.02222) and [Sebastian et al 2021](https://arxiv.org/abs/2102.12108)\n",
    "- Reference to [Gpytorch example](https://docs.gpytorch.ai/en/v1.12/examples/06_PyTorch_NN_Integration_DKL/Deep_Kernel_Learning_DenseNet_CIFAR_Tutorial.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install modules and start DigitalTwin microscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63289,
     "status": "ok",
     "timestamp": 1735229087621,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "M_lr7wqAnB6A",
    "outputId": "3bcce910-4642-497c-f877-9c3bf88c6f28"
   },
   "outputs": [],
   "source": [
    "#install\n",
    "!pip install -q botorch==0.12.0\n",
    "!pip install -q gpytorch==1.13\n",
    "!pip install -q git+https://github.com/pycroscopy/DTMicroscope.git\n",
    "!pip install -q h5py\n",
    "!pip install -q sidpy\n",
    "!pip install -q pyro5\n",
    "!pip install -q scifireaders\n",
    "!pip install -q pynsid\n",
    "!pip install -q pytemlib\n",
    "\n",
    "## start dtmic\n",
    "!run_server_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BEPS data - credits: yongtao liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37654,
     "status": "ok",
     "timestamp": 1735229130984,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "gy9_uYUGnB6A",
    "outputId": "1dd42aab-c0f8-46cf-ea9c-9992c1ef8624"
   },
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1UMub2L-9X8imtvTbz_aE4l4Gz-TW5Yd5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Preprocess data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1735229227077,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "AXYeV1M8nB6B",
    "outputId": "820e8dea-9b0c-4b93-938b-afc8539652be",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import sidpy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "from datetime import datetime\n",
    "import Pyro5.api\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pyNSID\n",
    "import SciFiReaders\n",
    "\n",
    "input_file = \"BEPS_1d7um_0009.h5\"\n",
    "h5_f = h5py.File(input_file, 'r+')\n",
    "sidpy.hdf.hdf_utils.print_tree(h5_f)\n",
    "\n",
    "\n",
    "sho_mat = h5_f['Measurement_000/Channel_000/Raw_Data-SHO_Fit_001/Fit']\n",
    "spec_val = h5_f['Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Values']\n",
    "pos_inds = h5_f['Measurement_000/Channel_000/Position_Indices']\n",
    "pos_dim_sizes = [np.max(pos_inds[:,0])+1, np.max(pos_inds[:,1]+1)]\n",
    "topo = h5_f['Measurement_000/Channel_001/Raw_Data/']['r']\n",
    "\n",
    "sho_mat_ndim = sho_mat[:].reshape(pos_dim_sizes[0], pos_dim_sizes[1], -1)\n",
    "amp_mat_ndim = sho_mat_ndim['Amplitude [V]']\n",
    "phase_mat_ndim = sho_mat_ndim['Phase [rad]']\n",
    "fre_mat_ndim = sho_mat_ndim['Frequency [Hz]']\n",
    "q_mat_ndim = sho_mat_ndim['Quality Factor']\n",
    "\n",
    "\n",
    "\n",
    "min_ = np.min(phase_mat_ndim)\n",
    "max_ = np.max(phase_mat_ndim)\n",
    "\n",
    "pha_correct = np.where(phase_mat_ndim > 1.5, phase_mat_ndim + (min_-max_), phase_mat_ndim)\n",
    "\n",
    "pha_correct = pha_correct + np.pi-0.55\n",
    "\n",
    "#separate on field and off field\n",
    "full_spec_len = sho_mat_ndim.shape[2]\n",
    "spec_len = int(full_spec_len/2)\n",
    "pix_x, pix_y = sho_mat_ndim.shape[0], sho_mat_ndim.shape[1]\n",
    "\n",
    "amp_off_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "pha_off_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "fre_off_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "\n",
    "amp_on_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "pha_on_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "fre_on_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "\n",
    "v_step = np.zeros(spec_len)\n",
    "\n",
    "for i in range (spec_len):\n",
    "  amp_off_field[:,:, i] = amp_mat_ndim[:,:,2*i+1]\n",
    "  pha_off_field[:,:, i] = pha_correct[:,:,2*i+1]\n",
    "  fre_off_field[:,:, i] = fre_mat_ndim[:,:,2*i+1]/1000\n",
    "  amp_on_field[:,:, i] = amp_mat_ndim[:,:,2*i]\n",
    "  pha_on_field[:,:, i] = pha_correct[:,:,2*i]\n",
    "  fre_on_field[:,:, i] = fre_mat_ndim[:,:,2*i]/1000\n",
    "  v_step[i] = spec_val[0,2*i]\n",
    "\n",
    "\n",
    "pola_off_field = amp_off_field*np.cos(pha_off_field)\n",
    "\n",
    "struc_img = amp_off_field.mean(2)\n",
    "\n",
    "\n",
    "## Cut a part for DKL exploration\n",
    "\n",
    "exp_data = pola_off_field[50:, 20:70]\n",
    "img_data = amp_off_field[50:, 20:70]\n",
    "\n",
    "struc_img = img_data.mean(2)\n",
    "plot_pixx1 = 10; plot_pixy1 = 30\n",
    "plot_pixx2 = 40; plot_pixy2 = 10\n",
    "\n",
    "norm_ = lambda x: (x - np.min(x)) / np.ptp(x)# or use:  norm_ = lambda x: (x - np.min(x)) / np.ptp(x) --> numpy-2.0 upgrade\n",
    "\n",
    "img = norm_(struc_img)\n",
    "spectra = norm_(exp_data)\n",
    "\n",
    "# coordinates = get_coord_grid(img, step = 1, return_dict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Prepare data for DigitalTwin Microscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1735229227847,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "lxDVHoF8nB6C",
    "outputId": "75f6815b-315c-4df2-c7f7-b9e8316c939f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = img\n",
    "spectrum_image = spectra\n",
    "## Set scale bar and energy axis of spectrum based on your data\n",
    "scale = 1\n",
    "energy_axis = np.linspace(0, 1, 256)\n",
    "\n",
    "data_sets = {'Channel_000': sidpy.Dataset.from_array(image , name = \"overview image\"),\n",
    "             'Channel_001': sidpy.Dataset.from_array(spectrum_image, name = \"spectrum image\")}\n",
    "\n",
    "data_sets['Channel_000'].data_type = 'image'\n",
    "data_sets['Channel_001'].data_type = 'spectral_image'\n",
    "\n",
    "data_sets['Channel_000'].set_dimension(0, sidpy.Dimension(np.arange(data_sets['Channel_001'].shape[0])*scale,\n",
    "                                          name='x', units='nm', quantity='Length',\n",
    "                                          dimension_type='spatial'))\n",
    "\n",
    "data_sets['Channel_000'].set_dimension(1, sidpy.Dimension(np.arange(data_sets['Channel_001'].shape[1])*scale,\n",
    "                                          'y', units='nm', quantity='Length',\n",
    "                                          dimension_type='spatial'))\n",
    "data_sets['Channel_001'].set_dimension(0, sidpy.Dimension(np.arange(data_sets['Channel_001'].shape[0])*scale,\n",
    "                                          name='x', units='nm', quantity='Length',\n",
    "                                          dimension_type='spatial'))\n",
    "data_sets['Channel_001'].set_dimension(1, sidpy.Dimension(np.arange(data_sets['Channel_001'].shape[1])*scale,\n",
    "                                          'y', units='nm', quantity='Length',\n",
    "                                          dimension_type='spatial'))\n",
    "data_sets['Channel_001'].set_dimension(2, sidpy.Dimension(energy_axis,\n",
    "                                          'energy_scale', units='ev', quantity='Energy',\n",
    "                                          dimension_type='spectral'))\n",
    "\n",
    "\n",
    "def save_dataset_dictionary(h5_file, datasets):\n",
    "    h5_measurement_group = sidpy.hdf.prov_utils.create_indexed_group(h5_file, 'Measurement_')\n",
    "    for key, dataset in datasets.items():\n",
    "        if key[-1] == '/':\n",
    "            key = key[:-1]\n",
    "        if isinstance(dataset, sidpy.Dataset):\n",
    "            h5_group = h5_measurement_group.create_group(key)\n",
    "            h5_dataset = pyNSID.hdf_io.write_nsid_dataset(dataset, h5_group)\n",
    "            dataset.h5_dataset = h5_dataset\n",
    "            h5_dataset.file.flush()\n",
    "        elif isinstance(dataset, dict):\n",
    "            sidpy.hdf.hdf_utils.write_dict_to_h5_group(h5_measurement_group, dataset, key)\n",
    "        else:\n",
    "            print('could not save item ', key, 'of dataset dictionary')\n",
    "    return h5_measurement_group\n",
    "\n",
    "dataset_name = 'yl_beps.h5'\n",
    "h5_file = h5py.File(dataset_name, mode='a')\n",
    "save_dataset_dictionary(h5_file, data_sets)\n",
    "h5_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi Objective Bayesian optimization with DKL\n",
    "- Note we use one surrogate to model each objective and then the next point is chosen based on the point which has highest value across both the surrogates(objectives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. DKL model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from botorch.models.model import Model\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qExpectedHypervolumeImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "# from botorch.utils.multi_objective.box_decomposition import NondominatedPartitioning\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.test_functions.multi_objective import BraninCurrin\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "from gpytorch.mlls.variational_elbo import VariationalELBO\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from torch.optim import Adam\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.acquisition.monte_carlo import qNoisyExpectedImprovement\n",
    "from botorch.optim import optimize_acqf_discrete\n",
    "from botorch.acquisition.multi_objective import ExpectedHypervolumeImprovement\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import FastNondominatedPartitioning\n",
    "from tqdm import tqdm\n",
    "import gpytorch\n",
    "from botorch.posteriors.gpytorch import GPyTorchPosterior\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional, Dict, Union, List\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Simple ConvNet for feature extraction\n",
    "class ConvNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_dim=32):\n",
    "        super(ConvNetFeatureExtractor, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.output_dim = output_dim\n",
    "        self.fc = None  # Placeholder for the fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3: # TODO: hacky way to make sure botorch acquisition function works\n",
    "            # flatten\n",
    "            batch_size, channel, mn = x.shape[0], x.shape[1] , x.shape[2]\n",
    "            d = math.sqrt(mn)      ## TODO: what if mn is not a perfect square?\n",
    "            x = x.reshape(int(batch_size), int(channel), int(d), int(d))\n",
    "        # Pass through the convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "        # If the fully connected layer is not defined yet, initialize it dynamically******************key\n",
    "        if self.fc is None:\n",
    "            flattened_size = x.view(x.size(0), -1).size(1)\n",
    "            device = x.device# TODO: better way to handle device\n",
    "            self.fc = nn.Linear(flattened_size, self.output_dim).to(device)  # Create fc layer on the correct device\n",
    "\n",
    "        # Flatten for fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# GP model with deep kernel using ConvNet feature extractor\n",
    "class GPModelDKL(ApproximateGP, Model):\n",
    "    def __init__(self, inducing_points, likelihood, feature_extractor=None, input_shape=(1, 5, 5)):\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Transform inducing points first\n",
    "        flat_inducing = self._flatten_input(inducing_points)\n",
    "        feature_inducing = feature_extractor(self._reshape_to_patch(flat_inducing))\n",
    "        # No need to reshape feature_inducing as it's already in correct shape\n",
    "        \n",
    "        variational_distribution = CholeskyVariationalDistribution(feature_inducing.size(0))\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, feature_inducing, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        \n",
    "        super().__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self._num_outputs = 1  # storing as private attribute\n",
    "        self.likelihood = likelihood\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def _flatten_input(self, x):\n",
    "        \"\"\"Flatten input to include channel dim\"\"\"\n",
    "        if x.ndim == 4:  # (batch, channel, height, width)\n",
    "            return x.reshape(x.size(0), 1, -1)\n",
    "        return x\n",
    "    \n",
    "    def _reshape_to_patch(self, x):\n",
    "        \"\"\"Reshape flattened input to patch format\"\"\"\n",
    "        if x.ndim == 3:  # (batch, channel, flattened)\n",
    "            batch_size = x.size(0)\n",
    "            return x.reshape(batch_size, *self.input_shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x should already be features at this point\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "    def __call__(self, x, use_feature_extractor=True, *args, **kwargs):\n",
    "        if use_feature_extractor:\n",
    "            # First ensure x is in patch format\n",
    "            x = self._reshape_to_patch(x)\n",
    "            # Get features - these will be [batch_size, feature_dim]\n",
    "            x = self.feature_extractor(x)\n",
    "            # No need to reshape after feature extraction\n",
    "        return super().__call__(x, *args, **kwargs)\n",
    "\n",
    "    def posterior(self, X, output_indices=None, observation_noise=False, *args, **kwargs):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Ensure correct shape for feature extraction\n",
    "            if X.ndim == 2:  # If input is (batch_size, flattened_dim)\n",
    "                X = X.unsqueeze(1)  # Add channel dim\n",
    "            X = self._reshape_to_patch(X)\n",
    "            # Features will be [batch_size, feature_dim]\n",
    "            features = self.feature_extractor(X)\n",
    "            dist = self.likelihood(self(features, use_feature_extractor=False))\n",
    "            # Ensure output has correct shape for MOBO\n",
    "            mean = dist.mean.unsqueeze(-1)  # Shape: [batch_size, 1]\n",
    "            variance = dist.variance.unsqueeze(-1)  # Shape: [batch_size, 1]\n",
    "            dist = gpytorch.distributions.MultivariateNormal(mean, torch.diag_embed(variance))\n",
    "        return GPyTorchPosterior(dist)\n",
    "    \n",
    "    @property\n",
    "    def num_outputs(self) -> int:\n",
    "        \"\"\"The number of outputs of the model.\"\"\"\n",
    "        return self._num_outputs\n",
    "\n",
    "    @property\n",
    "    def hparam_dict(self):\n",
    "        return {\n",
    "            \"likelihood.noise\": self.likelihood.noise.item(),\n",
    "            \"covar_module.base_kernel.outputscale\": self.covar_module.base_kernel.outputscale.item(),\n",
    "            \"mean_module.constant\": self.mean_module.constant.item(),\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Utility F:n's - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_data(data : np.ndarray) -> np.ndarray:  # Expected data type: torch.Tensor\n",
    "    \"\"\"Normalize data to the [0, 1] range.\"\"\"\n",
    "    return (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "\n",
    "def numpy_to_torch_for_conv(np_array) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a NumPy array of shape (batch_size, a, b) to a PyTorch tensor\n",
    "    with shape (batch_size, 1, a, b) for neural network use.\n",
    "\n",
    "    Parameters:\n",
    "        np_array (np.ndarray): Input NumPy array of shape (batch_size, a, b).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Converted PyTorch tensor of shape (batch_size, 1, a, b).\n",
    "    \"\"\"\n",
    "    # Check if input is a numpy array\n",
    "    if not isinstance(np_array, np.ndarray):\n",
    "        raise TypeError(\"Input must be a NumPy array.\")\n",
    "\n",
    "    # Convert to PyTorch tensor and add a channel dimension\n",
    "    # tensor = torch.from_numpy(np_array, dtype=torch.double)  # Convert to float tensor\n",
    "    tensor = torch.tensor(np_array, dtype=dtype)  # Convert to float tensor\n",
    "    tensor = tensor.unsqueeze(1)  # Add a channel dimension at index 1\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "######################atomai utils####################################\n",
    "#Credits Maxim Ziatdinov (https://github.com/ziatdinovmax): https://github.com/pycroscopy/atomai/blob/8db3e944cd9ece68c33c8e3fcca3ef3ce9a111ea/atomai/utils/img.py#L522\n",
    "\n",
    "def get_coord_grid(imgdata: np.ndarray, step: int,\n",
    "                   return_dict: bool = True\n",
    "                   ) -> Union[np.ndarray, Dict[int, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Generate a square coordinate grid for every image in a stack. Returns coordinates\n",
    "    in a dictionary format (same format as generated by atomnet.predictor)\n",
    "    that can be used as an input for utility functions extracting subimages\n",
    "    and atomstat.imlocal class\n",
    "\n",
    "    Args:\n",
    "        imgdata (numpy array): 2D or 3D numpy array\n",
    "        step (int): distance between grid points\n",
    "        return_dict (bool): returns coordiantes as a dictionary (same format as atomnet.predictor)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary or numpy array with coordinates\n",
    "    \"\"\"\n",
    "    if np.ndim(imgdata) == 2:\n",
    "        imgdata = np.expand_dims(imgdata, axis=0)\n",
    "    coord = []\n",
    "    for i in range(0, imgdata.shape[1], step):\n",
    "        for j in range(0, imgdata.shape[2], step):\n",
    "            coord.append(np.array([i, j]))\n",
    "    coord = np.array(coord)\n",
    "    if return_dict:\n",
    "        coord = np.concatenate((coord, np.zeros((coord.shape[0], 1))), axis=-1)\n",
    "        coordinates_dict = {i: coord for i in range(imgdata.shape[0])}\n",
    "        return coordinates_dict\n",
    "    coordinates = [coord for _ in range(imgdata.shape[0])]\n",
    "    return np.concatenate(coordinates, axis=0)\n",
    "\n",
    "def get_imgstack(imgdata: np.ndarray,\n",
    "                 coord: np.ndarray,\n",
    "                 r: int) -> Tuple[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts subimages centered at specified coordinates\n",
    "    for a single image\n",
    "\n",
    "    Args:\n",
    "        imgdata (3D numpy array):\n",
    "            Prediction of a neural network with dimensions\n",
    "            :math:`height \\\\times width \\\\times n channels`\n",
    "        coord (N x 2 numpy array):\n",
    "            (x, y) coordinates\n",
    "        r (int):\n",
    "            Window size\n",
    "\n",
    "    Returns:\n",
    "        2-element tuple containing\n",
    "\n",
    "        - Stack of subimages\n",
    "        - (x, y) coordinates of their centers\n",
    "    \"\"\"\n",
    "    img_cr_all = []\n",
    "    com = []\n",
    "    for c in coord:\n",
    "        cx = int(np.around(c[0]))\n",
    "        cy = int(np.around(c[1]))\n",
    "        if r % 2 != 0:\n",
    "            img_cr = np.copy(\n",
    "                imgdata[cx-r//2:cx+r//2+1,\n",
    "                        cy-r//2:cy+r//2+1])\n",
    "        else:\n",
    "            img_cr = np.copy(\n",
    "                imgdata[cx-r//2:cx+r//2,\n",
    "                        cy-r//2:cy+r//2])\n",
    "        if img_cr.shape[0:2] == (int(r), int(r)) and not np.isnan(img_cr).any():\n",
    "            img_cr_all.append(img_cr[None, ...])\n",
    "            com.append(c[None, ...])\n",
    "    if len(img_cr_all) == 0:\n",
    "        return None, None\n",
    "    img_cr_all = np.concatenate(img_cr_all, axis=0)\n",
    "    com = np.concatenate(com, axis=0)\n",
    "    return img_cr_all, com\n",
    "\n",
    "def extract_subimages(imgdata: np.ndarray,\n",
    "                      coordinates: Union[Dict[int, np.ndarray], np.ndarray],\n",
    "                      window_size: int, coord_class: int = 0) -> Tuple[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts subimages centered at certain atom class/type\n",
    "    (usually from a neural network output)\n",
    "\n",
    "    Args:\n",
    "        imgdata (numpy array):\n",
    "            4D stack of images (n, height, width, channel).\n",
    "            It is also possible to pass a single 2D image.\n",
    "        coordinates (dict or N x 2 numpy arry): Prediction from atomnet.locator\n",
    "            (can be from other source but must be in the same format)\n",
    "            Each element is a :math:`N \\\\times 3` numpy array,\n",
    "            where *N* is a number of detected atoms/defects,\n",
    "            the first 2 columns are *xy* coordinates\n",
    "            and the third columns is class (starts with 0).\n",
    "            It is also possible to pass N x 2 numpy array if the corresponding\n",
    "            imgdata is a single 2D image.\n",
    "        window_size (int):\n",
    "            Side of the square for subimage cropping\n",
    "        coord_class (int):\n",
    "            Class of atoms/defects around around which the subimages\n",
    "            will be cropped (3rd column in the atomnet.locator output)\n",
    "\n",
    "    Returns:\n",
    "        3-element tuple containing\n",
    "\n",
    "        - stack of subimages,\n",
    "        - (x, y) coordinates of their centers,\n",
    "        - frame number associated with each subimage\n",
    "    \"\"\"\n",
    "    if isinstance(coordinates, np.ndarray):\n",
    "        coordinates = np.concatenate((\n",
    "            coordinates, np.zeros((coordinates.shape[0], 1))), axis=-1)\n",
    "        coordinates = {0: coordinates}\n",
    "    if np.ndim(imgdata) == 2:\n",
    "        imgdata = imgdata[None, ..., None]\n",
    "    subimages_all, com_all, frames_all = [], [], []\n",
    "    for i, (img, coord) in enumerate(\n",
    "            zip(imgdata, coordinates.values())):\n",
    "        coord_i = coord[np.where(coord[:, 2] == coord_class)][:, :2]\n",
    "        stack_i, com_i = get_imgstack(img, coord_i, window_size)\n",
    "        if stack_i is None:\n",
    "            continue\n",
    "        subimages_all.append(stack_i)\n",
    "        com_all.append(com_i)\n",
    "        frames_all.append(np.ones(len(com_i), int) * i)\n",
    "    if len(subimages_all) > 0:\n",
    "        subimages_all = np.concatenate(subimages_all, axis=0)\n",
    "        com_all = np.concatenate(com_all, axis=0)\n",
    "        frames_all = np.concatenate(frames_all, axis=0)\n",
    "    return subimages_all, com_all, frames_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Utility F:n's - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1735229227847,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "jZhoMc9ZnB6C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#*********************************DTmic specific functions starts **********************************************#\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def black_box(index, indices_all, e1a, e1b) -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score simulates the blackbox function\n",
    "    \"\"\"\n",
    "\n",
    "    e_start,e_end = e1a, e1b\n",
    "    array_list, shape, dtype = mic_server.get_point_data(\n",
    "        spectrum_image_index=\"Channel_001\",\n",
    "        x=int(indices_all[index, 0]),######### TODO: check if x anf y needs to be flipped\n",
    "        y=int(indices_all[index, 1])\n",
    "    )\n",
    "    spectrum = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "    score = spectrum[e_start:e_end].sum()\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def black_box_loop_height(index, indices_all, e1a, e1b) -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score simulates the blackbox function\n",
    "    \"\"\"\n",
    "\n",
    "    e_start,e_end = e1a, e1b\n",
    "    array_list, shape, dtype = mic_server.get_point_data(\n",
    "        spectrum_image_index=\"Channel_001\",\n",
    "        x=int(indices_all[index, 0]),######### TODO: check if x anf y needs to be flipped\n",
    "        y=int(indices_all[index, 1])\n",
    "    )\n",
    "    spectrum = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "\n",
    "    def loop_height(raw_spec, cycle):\n",
    "        raw_spec_len = len(raw_spec)\n",
    "        cycle_len = int(raw_spec_len / cycle)\n",
    "        half_len = int(cycle_len / 2)\n",
    "        q_len = int(cycle_len / 4)\n",
    "        loop_top, loop_bottom = [], []\n",
    "        loop_top.append(raw_spec[q_len : q_len + half_len])\n",
    "        loop_top.append(raw_spec[q_len + 2*half_len : q_len + 3*half_len])\n",
    "        loop_top.append(raw_spec[q_len + 4*half_len : 2*q_len + 4*half_len])\n",
    "        loop_bottom.append(raw_spec[:q_len])\n",
    "        loop_bottom.append(raw_spec[q_len + half_len: q_len + 2*half_len])\n",
    "        loop_bottom.append(raw_spec[q_len + 3*half_len: q_len + 4*half_len])\n",
    "        loop_top = np.concatenate(loop_top)\n",
    "        loop_bottom = np.concatenate(loop_bottom)\n",
    "        return np.max(loop_top) - np.min(loop_bottom)\n",
    "\n",
    "    score = loop_height(raw_spec = spectrum, cycle = 3)# TODO: hard coded cycle now\n",
    "\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def black_box_loop_area(index, indices_all, e1a, e1b) -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score simulates the blackbox function\n",
    "    \"\"\"\n",
    "\n",
    "    e_start,e_end = e1a, e1b\n",
    "    array_list, shape, dtype = mic_server.get_point_data(\n",
    "        spectrum_image_index=\"Channel_001\",\n",
    "        x=int(indices_all[index, 0]),######### TODO: check if x anf y needs to be flipped\n",
    "        y=int(indices_all[index, 1])\n",
    "    )\n",
    "    spectrum = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "\n",
    "    def loop_area (raw_spec, cycle) :\n",
    "        raw_spec_len = len(raw_spec)\n",
    "        cycle_len = int(raw_spec_len / cycle)\n",
    "        half_len = int(cycle_len / 2)\n",
    "        q_len = int(cycle_len / 4)\n",
    "        loop_top, loop_bottom = [], []\n",
    "        loop_top.append(raw_spec[q_len : q_len + half_len])\n",
    "        loop_top.append(raw_spec[q_len + 2*half_len : q_len + 3*half_len])\n",
    "        loop_top.append(raw_spec[q_len + 4*half_len : 2*q_len + 4*half_len])\n",
    "        loop_bottom.append(raw_spec[:q_len])\n",
    "        loop_bottom.append(raw_spec[q_len + half_len: q_len + 2*half_len])\n",
    "        loop_bottom.append(raw_spec[q_len + 3*half_len: q_len + 4*half_len])\n",
    "        loop_top = np.concatenate(loop_top)\n",
    "        loop_bottom = np.concatenate(loop_bottom)\n",
    "        return np.abs(np.sum(loop_top)-np.sum(loop_bottom))\n",
    "\n",
    "    score = loop_area(raw_spec = spectrum, cycle = 3)# TODO: hard coded cycle now\n",
    "\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def black_box_positive_nucleation_bias(index, indices_all, e1a, e1b) -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score simulates the blackbox function\n",
    "    \"\"\"\n",
    "\n",
    "    e_start,e_end = e1a, e1b\n",
    "    array_list, shape, dtype = mic_server.get_point_data(\n",
    "        spectrum_image_index=\"Channel_001\",\n",
    "        x=int(indices_all[index, 0]),######### TODO: check if x anf y needs to be flipped\n",
    "        y=int(indices_all[index, 1])\n",
    "    )\n",
    "    spectrum = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "\n",
    "    def positive_nucleation_bias(raw_spec, cycle):\n",
    "        raw_spec_len = len(raw_spec)\n",
    "        cycle_len = int(raw_spec_len / cycle)\n",
    "        half_len = int(cycle_len / 2)\n",
    "        q_len = int(cycle_len / 4)\n",
    "        loop_top, loop_bottom = [], []\n",
    "        loop_top.append(raw_spec[q_len : q_len + half_len])\n",
    "        loop_top.append(raw_spec[q_len + 2*half_len : q_len + 3*half_len])\n",
    "        loop_top.append(raw_spec[q_len + 4*half_len : 2*q_len + 4*half_len])\n",
    "        loop_bottom.append(raw_spec[:q_len])\n",
    "        loop_bottom.append(raw_spec[q_len + half_len: q_len + 2*half_len])\n",
    "        loop_bottom.append(raw_spec[q_len + 3*half_len: q_len + 4*half_len])\n",
    "        loop_top = np.concatenate(loop_top)\n",
    "        loop_bottom = np.concatenate(loop_bottom)\n",
    "        return np.mean(loop_top) - np.mean(loop_bottom)\n",
    "\n",
    "    score = positive_nucleation_bias(raw_spec = spectrum, cycle = 3)# TODO: hard coded cycle now\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def black_box_negative_nucleation_bias(index, indices_all, e1a, e1b) -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score simulates the blackbox function\n",
    "    \"\"\"\n",
    "\n",
    "    e_start,e_end = e1a, e1b\n",
    "    array_list, shape, dtype = mic_server.get_point_data(\n",
    "        spectrum_image_index=\"Channel_001\",\n",
    "        x=int(indices_all[index, 0]),######### TODO: check if x anf y needs to be flipped\n",
    "        y=int(indices_all[index, 1])\n",
    "    )\n",
    "    spectrum = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "\n",
    "    def negative_nucleation_bias(raw_spec, cycle):\n",
    "        raw_spec_len = len(raw_spec)\n",
    "        cycle_len = int(raw_spec_len / cycle)\n",
    "        half_len = int(cycle_len / 2)\n",
    "        q_len = int(cycle_len / 4)\n",
    "        loop_top, loop_bottom = [], []\n",
    "        loop_top.append(raw_spec[q_len : q_len + half_len])\n",
    "        loop_top.append(raw_spec[q_len + 2*half_len : q_len + 3*half_len])\n",
    "        loop_top.append(raw_spec[q_len + 4*half_len : 2*q_len + 4*half_len])\n",
    "        loop_bottom.append(raw_spec[:q_len])\n",
    "        loop_bottom.append(raw_spec[q_len + half_len: q_len + 2*half_len])\n",
    "        loop_bottom.append(raw_spec[q_len + 3*half_len: q_len + 4*half_len])\n",
    "        loop_top = np.concatenate(loop_top)\n",
    "        loop_bottom = np.concatenate(loop_bottom)\n",
    "        return np.min(loop_top) - np.max(loop_bottom)\n",
    "\n",
    "    score = negative_nucleation_bias(raw_spec = spectrum, cycle = 3)# TODO: hard coded cycle now\n",
    "\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "## a) evaluations metrics like nlpd, mse ----\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Squared Error (MSE)\"\"\"\n",
    "    #Smaller values indicate better predictions.\n",
    "    #Squaring ensures that positive and negative errors don't cancel out.\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return mse\n",
    "\n",
    "def calculate_nlpd(y_true, y_pred_mean, y_pred_var):\n",
    "    \"\"\"Calculate Negative Log Predictive Density (NLPD)\"\"\"\n",
    "    #NLPD evaluates how well the predicted probability distribution matches the true values.\n",
    "    #Lower NLPD indicates a better match, accounting for both the mean and uncertainty.\n",
    "    nlpd = 0.5 * torch.log(2 * torch.pi * y_pred_var) + 0.5 * ((y_true - y_pred_mean) ** 2 / y_pred_var)\n",
    "    return nlpd.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Utility F:n's - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1735229227847,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "gQq7ZyWunB6D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_scores_for_patches(unacquired_indices, indices_all, e1a, e1b, black_box_fn = black_box) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the score for each patch using the black_box function.\n",
    "\n",
    "    Parameters:\n",
    "    - patches: Tensor of all data patches.\n",
    "\n",
    "    Returns:\n",
    "    - scores: List of scores for each patch.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i in unacquired_indices:\n",
    "        score = black_box_fn(i, indices_all, e1a, e1b)  # Calculate score for each patch\n",
    "        scores.append(score)\n",
    "    return torch.tensor(scores)  # Return as a tensor for compatibility\n",
    "\n",
    "def update_acquired(acquired_data, unacquired_indices, selected_indices, indices_all, e1a, e1b, black_box_fn = black_box) -> (np.array, list):\n",
    "    for idx in selected_indices:# TODO: It queries the black box everytime on already acquired points:\n",
    "        acquired_data[idx] = black_box_fn(idx, indices_all, e1a, e1b)\n",
    "    unacquired_indices = [idx for idx in unacquired_indices if idx not in selected_indices]\n",
    "\n",
    "\n",
    "    return acquired_data, unacquired_indices\n",
    "\n",
    "\n",
    "def load_image_and_features(img: np.ndarray , window_size : int) -> (np.ndarray, np.ndarray):\n",
    "    coordinates = get_coord_grid(img, step=1, return_dict=False)\n",
    "    features_all, coords, _ = extract_subimages(img, coordinates, window_size)\n",
    "    features_all = features_all[:, :, :, 0]\n",
    "    coords = np.array(coords, dtype=int)\n",
    "    norm_ = lambda x: (x - np.min(x)) / np.ptp(x) # or use:  norm_ = lambda x: (x - np.min(x)) / np.ptp(x) --> numpy-2.0 upgrade\n",
    "    features = norm_(features_all)\n",
    "    return features, coords# shapes (3366, 5, 5) and (3366, 2)\n",
    "\n",
    "\n",
    "def prepare_data_from_microscope(window_size: int) -> (np.ndarray, np.ndarray):\n",
    "    array_list, shape, dtype = mic_server.get_overview_image()\n",
    "    img = np.array(array_list, dtype=dtype).reshape(shape)#\n",
    "    features, indices_all = load_image_and_features(img, window_size)\n",
    "\n",
    "\n",
    "\n",
    "    return img, features, indices_all# shapes (55, 70), (3366, 5, 5) and (3366, 2)\n",
    "\n",
    "def get_spectrum_data(indices, energy_range, channel=\"Channel_001\") -> (np.array, int, int):\n",
    "    array_list, shape, dtype = mic_server.get_spectrum_image(spectrum_image_index=channel)\n",
    "    spectral_img = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "    array_list, shape, dtype = mic_server.get_spectrum_image_e_axis(spectrum_image_index=channel)\n",
    "    E_axis = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "    e_start, e_end = abs(E_axis - energy_range[0]).argmin(), abs(E_axis - energy_range[1]).argmin()\n",
    "    return spectral_img, e_start, e_end\n",
    "\n",
    "\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "\n",
    "def plot_pareto_front(acquired_data1, acquired_data2, step, save_path=None):\n",
    "    \"\"\"Enhanced Pareto front plotting with proper normalization and visualization\"\"\"\n",
    "    if not acquired_data1 or not acquired_data2:\n",
    "        log_with_context(\"Insufficient data to plot Pareto front.\")\n",
    "        return []\n",
    "    \n",
    "    # Extract and normalize objectives\n",
    "    common_indices = sorted(set(acquired_data1.keys()) & set(acquired_data2.keys()))\n",
    "    objectives = np.zeros((len(common_indices), 2))\n",
    "    \n",
    "    for i, idx in enumerate(common_indices):\n",
    "        objectives[i, 0] = acquired_data1[idx]\n",
    "        objectives[i, 1] = acquired_data2[idx]\n",
    "    \n",
    "    # Normalize objectives\n",
    "    objectives = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0))\n",
    "    objectives_tensor = torch.tensor(objectives, dtype=torch.float32)\n",
    "    \n",
    "    # Find Pareto optimal points\n",
    "    pareto_mask = is_non_dominated(objectives_tensor)\n",
    "    pareto_front = objectives[pareto_mask.numpy()]\n",
    "    \n",
    "    # Sort Pareto front for better visualization\n",
    "    pareto_front = pareto_front[pareto_front[:, 0].argsort()]\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(objectives[:, 0], objectives[:, 1], \n",
    "               c='lightgray', marker='o', label='All Points', alpha=0.5)\n",
    "    \n",
    "    # Plot Pareto front with connecting lines\n",
    "    plt.plot(pareto_front[:, 0], pareto_front[:, 1], \n",
    "            'r--', linewidth=2, label='Pareto Front')\n",
    "    plt.scatter(pareto_front[:, 0], pareto_front[:, 1], \n",
    "               c='red', marker='*', s=100, label='Pareto Optimal')\n",
    "    \n",
    "    # Annotate Pareto points\n",
    "    pareto_indices = [common_indices[i] for i, is_pareto in enumerate(pareto_mask.numpy()) if is_pareto]\n",
    "    for idx, point in zip(pareto_indices, pareto_front):\n",
    "        plt.annotate(f'#{idx}', \n",
    "                    (point[0], point[1]),\n",
    "                    xytext=(5, 5), \n",
    "                    textcoords='offset points',\n",
    "                    fontsize=8)\n",
    "    \n",
    "    plt.xlabel('Objective 1 (Normalized)')\n",
    "    plt.ylabel('Objective 2 (Normalized)')\n",
    "    plt.title(f'Pareto Front Evolution - Step {step + 1}')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add statistics text box\n",
    "    stats_text = f'Total Points: {len(common_indices)}\\n'\n",
    "    stats_text += f'Pareto Points: {len(pareto_indices)}'\n",
    "    plt.text(0.02, 0.98, stats_text,\n",
    "             transform=plt.gca().transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.8),\n",
    "             verticalalignment='top')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    return pareto_indices\n",
    "\n",
    "\n",
    "def embeddings_and_predictions(model, patches, device=\"cpu\") -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Get predictions from the trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    patches = patches.to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(patches)\n",
    "        embeddings = model.feature_extractor(patches).view(patches.size(0), -1).cpu().numpy()\n",
    "    return predictions, embeddings\n",
    "\n",
    "def train_model(acquired_data, patches, feature_extractor, \n",
    "                device=\"cpu\", num_epochs=50, log_interval=5,\n",
    "                scalarizer_zero=False, debug=False, window_size=5) -> ApproximateGP:\n",
    "\n",
    "    # Stack patches and prepare input\n",
    "    X_train_patches = torch.stack([patches[idx] for idx in acquired_data]).to(device)\n",
    "    X_train_flat = X_train_patches.reshape(X_train_patches.size(0),1, -1).to(device)  # Flatten for BoTorch compatibility\n",
    "    \n",
    "    y_train = torch.tensor(list(acquired_data.values()), dtype=torch.float32).to(device)\n",
    "    if scalarizer_zero:\n",
    "        y_train = torch.zeros_like(y_train)\n",
    "    else:\n",
    "        y_train = (y_train - y_train.min()) / (y_train.max() - y_train.min())\n",
    "\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "    # Initialize model with proper input shape\n",
    "    model = GPModelDKL(\n",
    "        inducing_points=X_train_flat[:10],  # Use flattened inducing points\n",
    "        likelihood=likelihood,\n",
    "        feature_extractor=feature_extractor,\n",
    "        input_shape=(1, window_size, window_size)  # Specify the original patch shape\n",
    "    ).to(device)\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs + 1), desc=\"Training Progress\"):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_flat)  # Pass flattened input\n",
    "        \n",
    "        \n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model\n",
    "\n",
    "# def train_models(acquired_data1, acquired_data2, acquired_data3, patches, feature_extractor1, feature_extractor2, feature_extractor3,\n",
    "#                 device=\"cpu\", window_size=5):\n",
    "#     # Train both models with flattened inputs\n",
    "#     model1 = train_model(acquired_data1, patches, feature_extractor1, device=device, window_size=window_size)\n",
    "#     model2 = train_model(acquired_data2, patches, feature_extractor2, device=device, window_size=window_size)\n",
    "#     model3 = train_model(acquired_data3, patches, feature_extractor3, device=device, window_size=window_size)\n",
    "\n",
    "    \n",
    "#     # Combine models for MOBO\n",
    "#     models = [model1, model2, model3]\n",
    "#     model = ModelListGP(*models)  # Properly initialize ModelListGP\n",
    "#     return model\n",
    "def train_models(acquired_data1, acquired_data2, patches, feature_extractor1, feature_extractor2,\n",
    "                device=\"cpu\", window_size=5):\n",
    "    # Train both models with flattened inputs\n",
    "\n",
    "    model1 = train_model(acquired_data1, patches, feature_extractor1, device=device, window_size=window_size)\n",
    "    model2 = train_model(acquired_data2, patches, feature_extractor2, device=device, window_size=window_size)\n",
    "    # model3 = train_model(acquired_data3, patches, feature_extractor3, device=device, window_size=window_size)\n",
    "\n",
    "    \n",
    "    # Combine models for MOBO\n",
    "    models = [model1, model2]\n",
    "    model = ModelListGP(*models)  # Properly initialize ModelListGP\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Bayesian optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1735229839511,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "YHfE0zAJnB6D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(config) -> None:\n",
    "    # Extract all configuration variables\n",
    "    seed = config[\"seed\"]\n",
    "    seed_pts = config[\"seed_pts\"]\n",
    "    budget = config[\"budget\"]\n",
    "    in_dir = config[\"in_dir\"]\n",
    "    out_dir_parent = config[\"out_dir_parent\"]\n",
    "    dataset_name = config[\"dataset_name\"]\n",
    "    device = config[\"device\"]\n",
    "    # initial_batch_size = config[\"initial_batch_size\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    normalize_data_flag = config[\"normalize_data\"]\n",
    "    window_size = config[\"window_size\"]\n",
    "    scal_pfm1 = config[\"scal_pfm1\"]\n",
    "    scal_pfm2 = config[\"scal_pfm2\"]\n",
    "    mobo_w1 = config[\"mobo_w1\"]\n",
    "    mobo_w2 = config[\"mobo_w2\"]\n",
    "    ## need scal1 and scal2\n",
    "\n",
    "    energy_range1 = [0, 1]# TODO: can be confusing as used in eels data - for now ignore for beps\n",
    "    energy_range2 = [0, 1]# TODO: can be confusing as used in eels data - for now ignore for beps\n",
    "\n",
    "    if scal_pfm1 is not None:## only for pfm: TODO : find better to accomodiate this\n",
    "        if scal_pfm1 == \"loop_area\":\n",
    "            black_box_fn1 = black_box_loop_height\n",
    "        \n",
    "        elif scal_pfm1 == \"loop_height\":\n",
    "            black_box_fn1 = black_box_loop_height\n",
    "            \n",
    "        elif scal_pfm1 == \"positive_nucleation_bias\":\n",
    "            black_box_fn1 = black_box_positive_nucleation_bias\n",
    "\n",
    "        elif scal_pfm1 ==  \"negative_nucleation_bias\":\n",
    "            black_box_fn1 = black_box_negative_nucleation_bias\n",
    "        \n",
    "    else :\n",
    "        black_box_fn1 = black_box\n",
    "\n",
    "    if scal_pfm2 is not None:## only for pfm: TODO : find better to accomodiate this\n",
    "        if scal_pfm2 == \"loop_area\":\n",
    "            black_box_fn2 = black_box_loop_height\n",
    "        \n",
    "        elif scal_pfm2 == \"loop_height\":\n",
    "            black_box_fn2 = black_box_loop_height\n",
    "            \n",
    "        elif scal_pfm2 == \"positive_nucleation_bias\":\n",
    "            black_box_fn2 = black_box_positive_nucleation_bias\n",
    "\n",
    "        elif scal_pfm2 ==  \"negative_nucleation_bias\":\n",
    "            black_box_fn2 = black_box_negative_nucleation_bias\n",
    "        \n",
    "    else :\n",
    "        black_box_fn2 = black_box\n",
    "    \n",
    "    scalarizer_zero = False # TODO: deafult value to zero -- so passed to train_model function --> better way to handel\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    res_dir = Path(out_dir_parent) / f\"MOBO_seed{seed}_Dataset_{dataset_name}_BO_{seed_pts}_epochs{num_epochs}_budget_{budget}_{scal_pfm1}_{scal_pfm2}_ws{window_size}_{timestamp}\"\n",
    "    res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Connect to the microscope server\n",
    "    uri = \"PYRO:microscope.server@localhost:9091\"\n",
    "    global mic_server # TODO: later see better way to do this\n",
    "    mic_server = Pyro5.api.Proxy(uri)\n",
    "\n",
    "    dataset_path = in_dir + \"/\" +  dataset_name\n",
    "    ### 2. Download data and register\n",
    "    # !wget https://github.com/pycroscopy/DTMicroscope/raw/utk/data/STEM/SI/test_stem.h5\n",
    "    mic_server.initialize_microscope(\"STEM\")\n",
    "    mic_server.register_data(dataset_path)\n",
    "\n",
    "\n",
    "    # Prepare features and indices from microscope image\n",
    "    img, features, indices_all = prepare_data_from_microscope(window_size=window_size)\n",
    "    ############################################\n",
    "    \n",
    "    \n",
    "    patches = numpy_to_torch_for_conv(features)\n",
    "\n",
    "    # Set up energy ranges for scalarizer extraction\n",
    "    spectral_img1, e1a, e1b = get_spectrum_data(indices_all, energy_range1)\n",
    "    spectral_img2, e2a, e2b = get_spectrum_data(indices_all, energy_range2)\n",
    "\n",
    "\n",
    "\n",
    "    patches = patches.to(device)\n",
    "\n",
    "    if normalize_data_flag:\n",
    "        patches = normalize_data(patches)\n",
    "\n",
    "    feature_extractor1 = ConvNetFeatureExtractor(input_channels=1, output_dim=2).to(device)\n",
    "    feature_extractor2 = ConvNetFeatureExtractor(input_channels=1, output_dim=2).to(device)\n",
    "\n",
    "    acquired_data1 = {}\n",
    "    acquired_data2 = {}\n",
    "    unacquired_indices1 = list(range(len(indices_all)))####### TODO: need to change later to use the indices_all\n",
    "    unacquired_indices2 = list(range(len(indices_all)))####### TODO: need to change later to use the indices_all\n",
    "\n",
    "    selected_indices1 = random.sample(unacquired_indices1, seed_pts)\n",
    "    selected_indices2 = selected_indices1\n",
    "     \n",
    "    seed_indices = selected_indices1\n",
    "    ######### queries spectrum_image\n",
    "    true_scalarizer1= calculate_scores_for_patches(unacquired_indices1, indices_all, e1a, e1b, black_box_fn=black_box_fn1)\n",
    "    true_scalarizer1= (true_scalarizer1 - true_scalarizer1.min()) / (true_scalarizer1.max() - true_scalarizer1.min())######## normalized\n",
    "    true_scalarizer2= calculate_scores_for_patches(unacquired_indices2, indices_all, e2a, e2b, black_box_fn=black_box_fn2)\n",
    "    true_scalarizer2= (true_scalarizer2 - true_scalarizer2.min()) / (true_scalarizer2.max() - true_scalarizer2.min())######## normalized\n",
    "\n",
    "        \n",
    "    ######### queries microscope \n",
    "    acquired_data2, unacquired_indices2 = update_acquired(acquired_data2, unacquired_indices2, selected_indices2, indices_all, e2a, e2b, black_box_fn= black_box_fn2)\n",
    "    acquired_data1, unacquired_indices1 = update_acquired(acquired_data1, unacquired_indices1, selected_indices1, indices_all, e1a, e1b, black_box_fn= black_box_fn1)\n",
    "\n",
    "    \n",
    "\n",
    "    from botorch.acquisition import LogExpectedImprovement #ExpectedImprovement\n",
    "    mean_y_pred_mean_al = []\n",
    "    mean_y_pred_variance_al = []\n",
    "    mae_list = []\n",
    "    nlpd_list = []\n",
    "    # Start Bayesian Optimization loop\n",
    "    for step in range(budget):\n",
    "\n",
    "        # Train the DKL model===========================NEW=====================\n",
    "        model = train_models(acquired_data1, acquired_data2, patches, \n",
    "                           feature_extractor1, feature_extractor2, device, window_size=window_size)\n",
    "        model.eval()\n",
    "\n",
    "        # Prepare candidate set (unacquired patches)\n",
    "        candidate_indices = unacquired_indices1\n",
    "        X_candidates = torch.stack([patches[idx] for idx in candidate_indices]).to(device)\n",
    "        X_candidates_flat = X_candidates.reshape(X_candidates.size(0), -1).to(device)\n",
    "\n",
    "        # Get training data and setup for EHVI\n",
    "        train_y1 = torch.tensor(list(acquired_data1.values()), dtype=torch.float32).to(device)\n",
    "        train_y2 = torch.tensor(list(acquired_data2.values()), dtype=torch.float32).to(device)\n",
    "        # train_y3 = torch.tensor(list(acquired_data3.values()), dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "        # Normalize and reshape training data\n",
    "        train_y1 = (train_y1 - train_y1.min()) / (train_y1.max() - train_y1.min())\n",
    "        train_y2 = (train_y2 - train_y2.min()) / (train_y2.max() - train_y2.min())\n",
    "        # train_y3 = (train_y3 - train_y3.min()) / (train_y3.max() - train_y3.min())\n",
    "        train_y = torch.stack([train_y1, train_y2], dim=-1)  # Shape: [n, 2]\n",
    "\n",
    "        # Define reference point and make sure it's 2D\n",
    "        ref_point = torch.zeros(2, device=device)  # For maximization\n",
    "\n",
    "        # Set up partitioning with properly shaped inputs\n",
    "        partitioning = FastNondominatedPartitioning(\n",
    "            ref_point=ref_point,\n",
    "            Y=train_y\n",
    "        )\n",
    "\n",
    "        # # Create acquisition function with correct shapes\n",
    "        # acq_func = ExpectedHypervolumeImprovement(\n",
    "        #     model=model,\n",
    "        #     ref_point=ref_point.clone(),\n",
    "        #     partitioning=partitioning\n",
    "        # )\n",
    "        qEHVI = qExpectedHypervolumeImprovement(\n",
    "            model=model,\n",
    "            ref_point=ref_point.clone(),\n",
    "            partitioning=partitioning,\n",
    "        )\n",
    "\n",
    "        # Ensure candidates are properly shaped\n",
    "        X_candidates = torch.stack([patches[idx] for idx in unacquired_indices1]).to(device)\n",
    "        X_candidates_flat = X_candidates.reshape(X_candidates.size(0), -1)\n",
    "\n",
    "        # Optimize with proper shapes\n",
    "        new_x, acq_value = optimize_acqf_discrete(\n",
    "            acq_function=qEHVI,\n",
    "            choices=X_candidates_flat,\n",
    "            q=1,\n",
    "        )\n",
    "        # acq = acq_value.cpu().detach().numpy() \n",
    "\n",
    "        # Map the selected candidate index to the original dataset index\n",
    "        # Convert back to index\n",
    "        selected_idx = torch.where(\n",
    "            (X_candidates_flat == new_x.view(1, -1)).all(dim=1)\n",
    "        )[0].item()\n",
    "        selected_indices = [candidate_indices[selected_idx]]\n",
    "\n",
    "        unacquired_indices1_temp = np.copy(unacquired_indices1)\n",
    "\n",
    "        # Update acquired data with new observations\n",
    "        acquired_data1, unacquired_indices1 = update_acquired(acquired_data1, unacquired_indices1, selected_indices, indices_all, e1a, e1b)\n",
    "        acquired_data2, unacquired_indices2 = update_acquired(acquired_data2, unacquired_indices2, selected_indices, indices_all, e2a, e2b)\n",
    "\n",
    "        # pareto_indices = plot_pareto_front(acquired_data1, acquired_data2, step, save_path=None)\n",
    "        print(f\"**************************done BO step {step +1}\", end='\\r')\n",
    "\n",
    "        #***********************************************************plotting in active-learning starts********************************************************************************************************\n",
    "        pareto_indices = plot_pareto_front(acquired_data1, acquired_data2, step)\n",
    "\n",
    "        model1 = model.models[0]\n",
    "        model2 = model.models[1]\n",
    "        import matplotlib.pyplot as plt\n",
    "        pred1, embeddings1 = embeddings_and_predictions(model1, patches, device)\n",
    "        pred2, embeddings2 = embeddings_and_predictions(model2, patches, device)\n",
    "        # ---------- Reward Predictions ----------\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "        sc0 = ax[0].scatter(indices_all[:, 1], indices_all[:, 0], c=pred1.mean.cpu().numpy(), cmap='viridis')\n",
    "        sc1 = ax[1].scatter(indices_all[:, 1], indices_all[:, 0], c=pred2.mean.cpu().numpy(), cmap='viridis')\n",
    "\n",
    "        ax[0].set_title('model1: predicted reward')\n",
    "        ax[1].set_title('model2: predicted reward')\n",
    "\n",
    "        for a, sc in zip(ax, [sc0, sc1]):\n",
    "            a.set_xlabel('X')\n",
    "            a.set_ylabel('Y')\n",
    "            a.invert_yaxis()\n",
    "            plt.colorbar(sc, ax=a, fraction=0.046, pad=0.04)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ---------- Spatial Embedding Channels ----------\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "        sc00 = ax[0, 0].scatter(indices_all[:, 1], indices_all[:, 0], c=embeddings1[:, 0], cmap='plasma')\n",
    "        sc01 = ax[0, 1].scatter(indices_all[:, 1], indices_all[:, 0], c=embeddings1[:, 1], cmap='plasma')\n",
    "        sc10 = ax[1, 0].scatter(indices_all[:, 1], indices_all[:, 0], c=embeddings2[:, 0], cmap='plasma')\n",
    "        sc11 = ax[1, 1].scatter(indices_all[:, 1], indices_all[:, 0], c=embeddings2[:, 1], cmap='plasma')\n",
    "\n",
    "        titles = [\n",
    "            \"model1: embedding[:, 0]\",\n",
    "            \"model1: embedding[:, 1]\",\n",
    "            \"model2: embedding[:, 0]\",\n",
    "            \"model2: embedding[:, 1]\",\n",
    "        ]\n",
    "\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                idx = i * 2 + j\n",
    "                ax[i, j].set_title(titles[idx])\n",
    "                ax[i, j].set_xlabel('X')\n",
    "                ax[i, j].set_ylabel('Y')\n",
    "                ax[i, j].invert_yaxis()\n",
    "                plt.colorbar([sc00, sc01, sc10, sc11][idx], ax=ax[i, j], fraction=0.046, pad=0.04)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # ---------- Latent Scatter Plots ----------\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        ax[0].scatter(embeddings1[:, 0], embeddings1[:, 1], alpha=0.7, edgecolor='k')\n",
    "        ax[1].scatter(embeddings2[:, 0], embeddings2[:, 1], alpha=0.7, edgecolor='k')\n",
    "\n",
    "        ax[0].set_title('model1: latent space')\n",
    "        ax[1].set_title('model2: latent space')\n",
    "\n",
    "        for a in ax:\n",
    "            a.set_xlabel('Embedding 1')\n",
    "            a.set_ylabel('Embedding 2')\n",
    "            a.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        #***********************************************************plotting in active-learning stops********************************************************************************************************\n",
    "\n",
    "        \n",
    "\n",
    "    # Save predictions as a .pkl file\n",
    "    Active_learning_statistics = {\n",
    "        \"img\": img,\n",
    "        \"features\": features,\n",
    "        \"indices_all\": np.array(indices_all),\n",
    "        \"seed_indices\": np.array(seed_indices),\n",
    "        \"unacquired_indices\": np.array(unacquired_indices1),\n",
    "        \"mean_y_pred_mean_al\": np.array(mean_y_pred_mean_al),\n",
    "        \"mean_y_pred_variance_al\": np.array(mean_y_pred_variance_al)\n",
    "        # \"mae\": np.array(mae_list),\n",
    "        # \"nlpd\": np.array(nlpd_list)\n",
    "                }\n",
    "\n",
    "    with open(Path(res_dir) / f'Active_learning_statistics.pkl', 'wb') as f:\n",
    "        pickle.dump(Active_learning_statistics, f)\n",
    "\n",
    "  \n",
    "    ##############################-> plot\n",
    "\n",
    "    predictions_data = Active_learning_statistics\n",
    "    # Extract necessary data\n",
    "    img = np.array(predictions_data[\"img\"])  # Image or grid for background visualization\n",
    "    seed_indices = np.array(predictions_data[\"seed_indices\"])  # Initial sampled indices (referring to positions in indices_all)\n",
    "    unacquired_indices = np.array(predictions_data[\"unacquired_indices\"])  # Remaining indices\n",
    "    indices_all = np.array(predictions_data[\"indices_all\"])  # All possible indices (coordinates)\n",
    "\n",
    "    # Map seed_indices and unacquired_indices to their coordinates in indices_all\n",
    "    seed_coords = indices_all[seed_indices]\n",
    "    unacquired_coords = indices_all[unacquired_indices]\n",
    "\n",
    "    # Calculate acquired indices as the complement of unacquired and seed indices\n",
    "    acquired_indices = np.setdiff1d(np.arange(indices_all.shape[0]), np.union1d(seed_indices, unacquired_indices), assume_unique=True)\n",
    "    acquired_coords = indices_all[acquired_indices]\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Display the image or grid as the background\n",
    "    plt.imshow(img, cmap=\"gray\", origin=\"lower\")\n",
    "\n",
    "    # Plot the seed points in blue\n",
    "    plt.scatter(seed_coords[:, 1], seed_coords[:, 0], c=\"b\", label=\"Seed Points\", marker=\"o\")\n",
    "\n",
    "    time_order = np.arange(len(acquired_coords))  # Create a sequence representing time\n",
    "    scatter = plt.scatter(acquired_coords[:, 1], acquired_coords[:, 0], c=time_order, cmap=\"bwr\", label=\"Acquired Points\", marker=\"x\")\n",
    "\n",
    "    # Plot the unacquired points in green\n",
    "    # plt.scatter(unacquired_coords[:, 1], unacquired_coords[:, 0], c=\"g\", label=\"Unacquired Points\", marker=\"+\")\n",
    "\n",
    "    # Set plot labels and legend\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "    plt.title(\"Active Learning Trajectory\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Add a colorbar and label it as \"Steps\"\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label(\"Steps\")\n",
    "\n",
    "\n",
    "    plt.savefig(Path(res_dir) / \"AL_traj.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f. Set parameters and Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0f3-LDlSnB6E",
    "outputId": "9fca0bc5-75a2-41f3-f777-98dfa1fbfe09"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"seed\" : 1, # for repeatibility\n",
    "        \"seed_pts\" : 10, # How many points you want to start your BO with?\n",
    "        \"budget\" : 50, # How many experimental budget you have?\n",
    "        \"in_dir\": \".\", # recommended : leave as is\n",
    "        \"out_dir_parent\": \"out\", # recommended : leave as is\n",
    "        \"dataset_name\": \"yl_beps.h5\", # name of data to be loaded in DTmicroscope\n",
    "        \"device\": \"cuda\",\n",
    "        \"num_epochs\": 100, # Number of epoch the dkl model trains at each experimental step - Might need tuning based on data\n",
    "        \"normalize_data\": True, \n",
    "        \"window_size\": 16, # For square patches - structure property relationship\n",
    "        \"scal_pfm1\": \"positive_nucleation_bias\", # What physics interested in? options on this data: \"loop_area\", \"loop_height\", \"positive_nucleation_bias\", \"negative_nucleation_bias\",\n",
    "        \"scal_pfm2\": \"negative_nucleation_bias\", # What physics interested in? options on this data: \"loop_area\", \"loop_height\", \"positive_nucleation_bias\", \"negative_nucleation_bias\"\n",
    "        \"mobo_w1\": 1,########----------> weight of first objective on the acquisiton values\n",
    "        \"mobo_w2\": 1,########----------> weight of second objective on the acquisiton values\n",
    "        }\n",
    "run(config)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
