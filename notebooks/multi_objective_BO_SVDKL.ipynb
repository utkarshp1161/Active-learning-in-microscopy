{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2axl6kznB5_"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/utkarshp1161/Active-learning-in-microscopy/blob/main/notebooks/multi_objective_BO_SVDKL.ipynb)\n",
    "\n",
    "# Multi-objective Active learning using DigitalTwin microscope: Stochastic Variational Deep kernel learning in Gpytorch and BO loop in Botorch. [Recommended to take a GPU instance]\n",
    "- For single objective please [see](https://github.com/utkarshp1161/Active-learning-in-microscopy/blob/main/notebooks/single_objective_BO_SVDKL.ipynb) \n",
    "\n",
    "Prepared by [Utkarsh Pratiush](https://github.com/utkarshp1161)\n",
    "- Get in touch if any doubts or discussion [utkarshp1161@gmail.com]\n",
    "- For reading related to this notebook please refer to [wilson et al 2016](https://arxiv.org/abs/1611.00336), [wilson et al 2015](https://arxiv.org/abs/1511.02222) and [Sebastian et al 2021](https://arxiv.org/abs/2102.12108)\n",
    "- Reference to [Gpytorch example](https://docs.gpytorch.ai/en/v1.12/examples/06_PyTorch_NN_Integration_DKL/Deep_Kernel_Learning_DenseNet_CIFAR_Tutorial.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install modules and start DigitalTwin microscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63289,
     "status": "ok",
     "timestamp": 1735229087621,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "M_lr7wqAnB6A",
    "outputId": "3bcce910-4642-497c-f877-9c3bf88c6f28"
   },
   "outputs": [],
   "source": [
    "#install\n",
    "!pip install botorch==0.12.0\n",
    "!pip install gpytorch==1.13\n",
    "!pip install git+https://github.com/pycroscopy/DTMicroscope.git\n",
    "!pip install h5py\n",
    "!pip install sidpy\n",
    "!pip install -q pyro5\n",
    "!pip install -q scifireaders\n",
    "!pip install -q pynsid\n",
    "\n",
    "## start dtmic\n",
    "!run_server_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BEPS data - credits: yongtao liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37654,
     "status": "ok",
     "timestamp": 1735229130984,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "gy9_uYUGnB6A",
    "outputId": "1dd42aab-c0f8-46cf-ea9c-9992c1ef8624"
   },
   "outputs": [],
   "source": [
    "!gdown https://drive.google.com/uc?id=1x3Nw7ZQoZZlxDusDulgKLt2P0Wu3SJPo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Preprocess data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1735229227077,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "AXYeV1M8nB6B",
    "outputId": "820e8dea-9b0c-4b93-938b-afc8539652be"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import sidpy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "from datetime import datetime\n",
    "import Pyro5.api\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pyNSID\n",
    "import SciFiReaders\n",
    "\n",
    "input_file = \"BEPS_1d7um_0009.h5\"\n",
    "h5_f = h5py.File(input_file, 'r+')\n",
    "sidpy.hdf.hdf_utils.print_tree(h5_f)\n",
    "\n",
    "\n",
    "sho_mat = h5_f['Measurement_000/Channel_000/Raw_Data-SHO_Fit_001/Fit']\n",
    "spec_val = h5_f['Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Values']\n",
    "pos_inds = h5_f['Measurement_000/Channel_000/Position_Indices']\n",
    "pos_dim_sizes = [np.max(pos_inds[:,0])+1, np.max(pos_inds[:,1]+1)]\n",
    "topo = h5_f['Measurement_000/Channel_001/Raw_Data/']['r']\n",
    "\n",
    "sho_mat_ndim = sho_mat[:].reshape(pos_dim_sizes[0], pos_dim_sizes[1], -1)\n",
    "amp_mat_ndim = sho_mat_ndim['Amplitude [V]']\n",
    "phase_mat_ndim = sho_mat_ndim['Phase [rad]']\n",
    "fre_mat_ndim = sho_mat_ndim['Frequency [Hz]']\n",
    "q_mat_ndim = sho_mat_ndim['Quality Factor']\n",
    "\n",
    "\n",
    "\n",
    "min_ = np.min(phase_mat_ndim)\n",
    "max_ = np.max(phase_mat_ndim)\n",
    "\n",
    "pha_correct = np.where(phase_mat_ndim > 1.5, phase_mat_ndim + (min_-max_), phase_mat_ndim)\n",
    "\n",
    "pha_correct = pha_correct + np.pi-0.55\n",
    "\n",
    "#separate on field and off field\n",
    "full_spec_len = sho_mat_ndim.shape[2]\n",
    "spec_len = int(full_spec_len/2)\n",
    "pix_x, pix_y = sho_mat_ndim.shape[0], sho_mat_ndim.shape[1]\n",
    "\n",
    "amp_off_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "pha_off_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "fre_off_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "\n",
    "amp_on_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "pha_on_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "fre_on_field = np.zeros((pix_x, pix_y, spec_len))\n",
    "\n",
    "v_step = np.zeros(spec_len)\n",
    "\n",
    "for i in range (spec_len):\n",
    "  amp_off_field[:,:, i] = amp_mat_ndim[:,:,2*i+1]\n",
    "  pha_off_field[:,:, i] = pha_correct[:,:,2*i+1]\n",
    "  fre_off_field[:,:, i] = fre_mat_ndim[:,:,2*i+1]/1000\n",
    "  amp_on_field[:,:, i] = amp_mat_ndim[:,:,2*i]\n",
    "  pha_on_field[:,:, i] = pha_correct[:,:,2*i]\n",
    "  fre_on_field[:,:, i] = fre_mat_ndim[:,:,2*i]/1000\n",
    "  v_step[i] = spec_val[0,2*i]\n",
    "\n",
    "\n",
    "pola_off_field = amp_off_field*np.cos(pha_off_field)\n",
    "\n",
    "struc_img = amp_off_field.mean(2)\n",
    "\n",
    "\n",
    "## Cut a part for DKL exploration\n",
    "\n",
    "exp_data = pola_off_field[50:, 20:70]\n",
    "img_data = amp_off_field[50:, 20:70]\n",
    "\n",
    "struc_img = img_data.mean(2)\n",
    "plot_pixx1 = 10; plot_pixy1 = 30\n",
    "plot_pixx2 = 40; plot_pixy2 = 10\n",
    "\n",
    "norm_ = lambda x: (x - x.min()) / x.ptp()\n",
    "\n",
    "img = norm_(struc_img)\n",
    "spectra = norm_(exp_data)\n",
    "\n",
    "# coordinates = get_coord_grid(img, step = 1, return_dict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Prepare data for DigitalTwin Microscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1735229227847,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "lxDVHoF8nB6C",
    "outputId": "75f6815b-315c-4df2-c7f7-b9e8316c939f"
   },
   "outputs": [],
   "source": [
    "image = img\n",
    "spectrum_image = spectra\n",
    "## Set scale bar and energy axis of spectrum based on your data\n",
    "scale = 1\n",
    "energy_axis = np.linspace(0, 1, 256)\n",
    "\n",
    "data_sets = {'Channel_000': sidpy.Dataset.from_array(image , name = \"overview image\"),\n",
    "             'Channel_001': sidpy.Dataset.from_array(spectrum_image, name = \"spectrum image\")}\n",
    "\n",
    "data_sets['Channel_000'].data_type = 'image'\n",
    "data_sets['Channel_001'].data_type = 'spectral_image'\n",
    "\n",
    "data_sets['Channel_000'].set_dimension(0, sidpy.Dimension(np.arange(data_sets['Channel_001'].shape[0])*scale,\n",
    "                                          name='x', units='nm', quantity='Length',\n",
    "                                          dimension_type='spatial'))\n",
    "\n",
    "data_sets['Channel_000'].set_dimension(1, sidpy.Dimension(np.arange(data_sets['Channel_001'].shape[1])*scale,\n",
    "                                          'y', units='nm', quantity='Length',\n",
    "                                          dimension_type='spatial'))\n",
    "data_sets['Channel_001'].set_dimension(0, sidpy.Dimension(np.arange(data_sets['Channel_001'].shape[0])*scale,\n",
    "                                          name='x', units='nm', quantity='Length',\n",
    "                                          dimension_type='spatial'))\n",
    "data_sets['Channel_001'].set_dimension(1, sidpy.Dimension(np.arange(data_sets['Channel_001'].shape[1])*scale,\n",
    "                                          'y', units='nm', quantity='Length',\n",
    "                                          dimension_type='spatial'))\n",
    "data_sets['Channel_001'].set_dimension(2, sidpy.Dimension(energy_axis,\n",
    "                                          'energy_scale', units='ev', quantity='Energy',\n",
    "                                          dimension_type='spectral'))\n",
    "\n",
    "\n",
    "def save_dataset_dictionary(h5_file, datasets):\n",
    "    h5_measurement_group = sidpy.hdf.prov_utils.create_indexed_group(h5_file, 'Measurement_')\n",
    "    for key, dataset in datasets.items():\n",
    "        if key[-1] == '/':\n",
    "            key = key[:-1]\n",
    "        if isinstance(dataset, sidpy.Dataset):\n",
    "            h5_group = h5_measurement_group.create_group(key)\n",
    "            h5_dataset = pyNSID.hdf_io.write_nsid_dataset(dataset, h5_group)\n",
    "            dataset.h5_dataset = h5_dataset\n",
    "            h5_dataset.file.flush()\n",
    "        elif isinstance(dataset, dict):\n",
    "            sidpy.hdf.hdf_utils.write_dict_to_h5_group(h5_measurement_group, dataset, key)\n",
    "        else:\n",
    "            print('could not save item ', key, 'of dataset dictionary')\n",
    "    return h5_measurement_group\n",
    "\n",
    "dataset_name = 'yl_beps.h5'\n",
    "h5_file = h5py.File(dataset_name, mode='a')\n",
    "save_dataset_dictionary(h5_file, data_sets)\n",
    "h5_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi Objective Bayesian optimization with DKL\n",
    "- Note we use one surrogate to model each objective and then the next point is chosen based on the point which has highest value across both the surrogates(objectives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. DKL model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from botorch.posteriors.gpytorch import GPyTorchPosterior\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Tuple, Optional, Dict, Union, List\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Simple ConvNet for feature extraction\n",
    "class ConvNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_dim=32):\n",
    "        super(ConvNetFeatureExtractor, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.output_dim = output_dim\n",
    "        self.fc = None  # Placeholder for the fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3: # TODO: hacky way to make sure botorch acquisition function works\n",
    "            # flatten\n",
    "            batch_size, channel, mn = x.shape[0], x.shape[1] , x.shape[2]\n",
    "            d = math.sqrt(mn)      ## TODO: what if mn is not a perfect square?\n",
    "            x = x.reshape(int(batch_size), int(channel), int(d), int(d))\n",
    "        # Pass through the convolutional layers\n",
    "        x = self.conv_layers(x)\n",
    "\n",
    "\n",
    "        # If the fully connected layer is not defined yet, initialize it dynamically******************key\n",
    "        if self.fc is None:\n",
    "            flattened_size = x.view(x.size(0), -1).size(1)\n",
    "            device = x.device# TODO: better way to handle device\n",
    "            self.fc = nn.Linear(flattened_size, self.output_dim).to(device)  # Create fc layer on the correct device\n",
    "\n",
    "        # Flatten for fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# GP model with deep kernel using ConvNet feature extractor\n",
    "class GPModelDKL(ApproximateGP):\n",
    "    def __init__(self, inducing_points, likelihood, feature_extractor=None):\n",
    "        if feature_extractor is None:\n",
    "            feature_extractor = ConvNetFeatureExtractor(\n",
    "                input_channels=1,  # Set according to your image channels\n",
    "                output_dim=32      # Set as per the final feature dimension\n",
    "            ).to(inducing_points.device)\n",
    "        else:\n",
    "            feature_extractor = feature_extractor.to(inducing_points.device)\n",
    "\n",
    "        # Transform inducing points with ConvNet\n",
    "        inducing_points = feature_extractor(inducing_points)\n",
    "\n",
    "        # Variational setup\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            inducing_points.size(0)\n",
    "        )\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "\n",
    "        super(GPModelDKL, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.num_outputs = 1  # must be one\n",
    "        self.likelihood = likelihood\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, x, use_feature_extractor=True, *args, **kwargs):\n",
    "        ## TODO: to make it compatible with botorch acquisition function we need it to make patches internally from flattened patches\n",
    "        if use_feature_extractor:\n",
    "            if len(x.shape) == 3:\n",
    "                # flatten\n",
    "                batch_size, channel, mn = x.shape[0], x.shape[1] , x.shape[2]\n",
    "                d = math.sqrt(mn)      ## TODO: what if mn is not a perfect square?\n",
    "                x = x.reshape(int(batch_size), int(channel), int(d), int(d))\n",
    "            x = self.feature_extractor(x)\n",
    "        return super().__call__(x, *args, **kwargs)\n",
    "\n",
    "    def posterior(self, X, output_indices=None, observation_noise=False, *args, **kwargs) -> GPyTorchPosterior:\n",
    "        self.eval()\n",
    "        self.likelihood.eval()\n",
    "        dist = self.likelihood(self(X))\n",
    "        return GPyTorchPosterior(dist)\n",
    "\n",
    "    @property\n",
    "    def hparam_dict(self):\n",
    "        return {\n",
    "            \"likelihood.noise\": self.likelihood.noise.item(),\n",
    "            \"covar_module.base_kernel.outputscale\": self.covar_module.base_kernel.outputscale.item(),\n",
    "            \"mean_module.constant\": self.mean_module.constant.item(),\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Utility F:n's - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data : np.ndarray) -> np.ndarray:  # Expected data type: torch.Tensor\n",
    "    \"\"\"Normalize data to the [0, 1] range.\"\"\"\n",
    "    return (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "\n",
    "def numpy_to_torch_for_conv(np_array) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Converts a NumPy array of shape (batch_size, a, b) to a PyTorch tensor\n",
    "    with shape (batch_size, 1, a, b) for neural network use.\n",
    "\n",
    "    Parameters:\n",
    "        np_array (np.ndarray): Input NumPy array of shape (batch_size, a, b).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Converted PyTorch tensor of shape (batch_size, 1, a, b).\n",
    "    \"\"\"\n",
    "    # Check if input is a numpy array\n",
    "    if not isinstance(np_array, np.ndarray):\n",
    "        raise TypeError(\"Input must be a NumPy array.\")\n",
    "\n",
    "    # Convert to PyTorch tensor and add a channel dimension\n",
    "    tensor = torch.from_numpy(np_array).float()  # Convert to float tensor\n",
    "    tensor = tensor.unsqueeze(1)  # Add a channel dimension at index 1\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "######################atomai utils####################################\n",
    "#Credits Maxim Ziatdinov (https://github.com/ziatdinovmax): https://github.com/pycroscopy/atomai/blob/8db3e944cd9ece68c33c8e3fcca3ef3ce9a111ea/atomai/utils/img.py#L522\n",
    "\n",
    "def get_coord_grid(imgdata: np.ndarray, step: int,\n",
    "                   return_dict: bool = True\n",
    "                   ) -> Union[np.ndarray, Dict[int, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Generate a square coordinate grid for every image in a stack. Returns coordinates\n",
    "    in a dictionary format (same format as generated by atomnet.predictor)\n",
    "    that can be used as an input for utility functions extracting subimages\n",
    "    and atomstat.imlocal class\n",
    "\n",
    "    Args:\n",
    "        imgdata (numpy array): 2D or 3D numpy array\n",
    "        step (int): distance between grid points\n",
    "        return_dict (bool): returns coordiantes as a dictionary (same format as atomnet.predictor)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary or numpy array with coordinates\n",
    "    \"\"\"\n",
    "    if np.ndim(imgdata) == 2:\n",
    "        imgdata = np.expand_dims(imgdata, axis=0)\n",
    "    coord = []\n",
    "    for i in range(0, imgdata.shape[1], step):\n",
    "        for j in range(0, imgdata.shape[2], step):\n",
    "            coord.append(np.array([i, j]))\n",
    "    coord = np.array(coord)\n",
    "    if return_dict:\n",
    "        coord = np.concatenate((coord, np.zeros((coord.shape[0], 1))), axis=-1)\n",
    "        coordinates_dict = {i: coord for i in range(imgdata.shape[0])}\n",
    "        return coordinates_dict\n",
    "    coordinates = [coord for _ in range(imgdata.shape[0])]\n",
    "    return np.concatenate(coordinates, axis=0)\n",
    "\n",
    "def get_imgstack(imgdata: np.ndarray,\n",
    "                 coord: np.ndarray,\n",
    "                 r: int) -> Tuple[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts subimages centered at specified coordinates\n",
    "    for a single image\n",
    "\n",
    "    Args:\n",
    "        imgdata (3D numpy array):\n",
    "            Prediction of a neural network with dimensions\n",
    "            :math:`height \\\\times width \\\\times n channels`\n",
    "        coord (N x 2 numpy array):\n",
    "            (x, y) coordinates\n",
    "        r (int):\n",
    "            Window size\n",
    "\n",
    "    Returns:\n",
    "        2-element tuple containing\n",
    "\n",
    "        - Stack of subimages\n",
    "        - (x, y) coordinates of their centers\n",
    "    \"\"\"\n",
    "    img_cr_all = []\n",
    "    com = []\n",
    "    for c in coord:\n",
    "        cx = int(np.around(c[0]))\n",
    "        cy = int(np.around(c[1]))\n",
    "        if r % 2 != 0:\n",
    "            img_cr = np.copy(\n",
    "                imgdata[cx-r//2:cx+r//2+1,\n",
    "                        cy-r//2:cy+r//2+1])\n",
    "        else:\n",
    "            img_cr = np.copy(\n",
    "                imgdata[cx-r//2:cx+r//2,\n",
    "                        cy-r//2:cy+r//2])\n",
    "        if img_cr.shape[0:2] == (int(r), int(r)) and not np.isnan(img_cr).any():\n",
    "            img_cr_all.append(img_cr[None, ...])\n",
    "            com.append(c[None, ...])\n",
    "    if len(img_cr_all) == 0:\n",
    "        return None, None\n",
    "    img_cr_all = np.concatenate(img_cr_all, axis=0)\n",
    "    com = np.concatenate(com, axis=0)\n",
    "    return img_cr_all, com\n",
    "\n",
    "def extract_subimages(imgdata: np.ndarray,\n",
    "                      coordinates: Union[Dict[int, np.ndarray], np.ndarray],\n",
    "                      window_size: int, coord_class: int = 0) -> Tuple[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts subimages centered at certain atom class/type\n",
    "    (usually from a neural network output)\n",
    "\n",
    "    Args:\n",
    "        imgdata (numpy array):\n",
    "            4D stack of images (n, height, width, channel).\n",
    "            It is also possible to pass a single 2D image.\n",
    "        coordinates (dict or N x 2 numpy arry): Prediction from atomnet.locator\n",
    "            (can be from other source but must be in the same format)\n",
    "            Each element is a :math:`N \\\\times 3` numpy array,\n",
    "            where *N* is a number of detected atoms/defects,\n",
    "            the first 2 columns are *xy* coordinates\n",
    "            and the third columns is class (starts with 0).\n",
    "            It is also possible to pass N x 2 numpy array if the corresponding\n",
    "            imgdata is a single 2D image.\n",
    "        window_size (int):\n",
    "            Side of the square for subimage cropping\n",
    "        coord_class (int):\n",
    "            Class of atoms/defects around around which the subimages\n",
    "            will be cropped (3rd column in the atomnet.locator output)\n",
    "\n",
    "    Returns:\n",
    "        3-element tuple containing\n",
    "\n",
    "        - stack of subimages,\n",
    "        - (x, y) coordinates of their centers,\n",
    "        - frame number associated with each subimage\n",
    "    \"\"\"\n",
    "    if isinstance(coordinates, np.ndarray):\n",
    "        coordinates = np.concatenate((\n",
    "            coordinates, np.zeros((coordinates.shape[0], 1))), axis=-1)\n",
    "        coordinates = {0: coordinates}\n",
    "    if np.ndim(imgdata) == 2:\n",
    "        imgdata = imgdata[None, ..., None]\n",
    "    subimages_all, com_all, frames_all = [], [], []\n",
    "    for i, (img, coord) in enumerate(\n",
    "            zip(imgdata, coordinates.values())):\n",
    "        coord_i = coord[np.where(coord[:, 2] == coord_class)][:, :2]\n",
    "        stack_i, com_i = get_imgstack(img, coord_i, window_size)\n",
    "        if stack_i is None:\n",
    "            continue\n",
    "        subimages_all.append(stack_i)\n",
    "        com_all.append(com_i)\n",
    "        frames_all.append(np.ones(len(com_i), int) * i)\n",
    "    if len(subimages_all) > 0:\n",
    "        subimages_all = np.concatenate(subimages_all, axis=0)\n",
    "        com_all = np.concatenate(com_all, axis=0)\n",
    "        frames_all = np.concatenate(frames_all, axis=0)\n",
    "    return subimages_all, com_all, frames_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Utility F:n's - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1735229227847,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "jZhoMc9ZnB6C"
   },
   "outputs": [],
   "source": [
    "#*********************************DTmic specific functions starts **********************************************#\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def black_box(index, indices_all, e1a, e1b) -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score simulates the blackbox function\n",
    "    \"\"\"\n",
    "\n",
    "    e_start,e_end = e1a, e1b\n",
    "    array_list, shape, dtype = mic_server.get_point_data(\n",
    "        spectrum_image_index=\"Channel_001\",\n",
    "        x=int(indices_all[index, 0]),######### TODO: check if x anf y needs to be flipped\n",
    "        y=int(indices_all[index, 1])\n",
    "    )\n",
    "    spectrum = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "    score = spectrum[e_start:e_end].sum()\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def black_box_loop_height(index, indices_all, e1a, e1b) -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score simulates the blackbox function\n",
    "    \"\"\"\n",
    "\n",
    "    e_start,e_end = e1a, e1b\n",
    "    array_list, shape, dtype = mic_server.get_point_data(\n",
    "        spectrum_image_index=\"Channel_001\",\n",
    "        x=int(indices_all[index, 0]),######### TODO: check if x anf y needs to be flipped\n",
    "        y=int(indices_all[index, 1])\n",
    "    )\n",
    "    spectrum = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "\n",
    "    def loop_height(raw_spec, cycle):\n",
    "        raw_spec_len = len(raw_spec)\n",
    "        cycle_len = int(raw_spec_len / cycle)\n",
    "        half_len = int(cycle_len / 2)\n",
    "        q_len = int(cycle_len / 4)\n",
    "        loop_top, loop_bottom = [], []\n",
    "        loop_top.append(raw_spec[q_len : q_len + half_len])\n",
    "        loop_top.append(raw_spec[q_len + 2*half_len : q_len + 3*half_len])\n",
    "        loop_top.append(raw_spec[q_len + 4*half_len : 2*q_len + 4*half_len])\n",
    "        loop_bottom.append(raw_spec[:q_len])\n",
    "        loop_bottom.append(raw_spec[q_len + half_len: q_len + 2*half_len])\n",
    "        loop_bottom.append(raw_spec[q_len + 3*half_len: q_len + 4*half_len])\n",
    "        loop_top = np.concatenate(loop_top)\n",
    "        loop_bottom = np.concatenate(loop_bottom)\n",
    "        return np.max(loop_top) - np.min(loop_bottom)\n",
    "\n",
    "    score = loop_height(raw_spec = spectrum, cycle = 3)# TODO: hard coded cycle now\n",
    "\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def black_box_loop_area(index, indices_all, e1a, e1b) -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score simulates the blackbox function\n",
    "    \"\"\"\n",
    "\n",
    "    e_start,e_end = e1a, e1b\n",
    "    array_list, shape, dtype = mic_server.get_point_data(\n",
    "        spectrum_image_index=\"Channel_001\",\n",
    "        x=int(indices_all[index, 0]),######### TODO: check if x anf y needs to be flipped\n",
    "        y=int(indices_all[index, 1])\n",
    "    )\n",
    "    spectrum = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "\n",
    "    def loop_area (raw_spec, cycle) :\n",
    "        raw_spec_len = len(raw_spec)\n",
    "        cycle_len = int(raw_spec_len / cycle)\n",
    "        half_len = int(cycle_len / 2)\n",
    "        q_len = int(cycle_len / 4)\n",
    "        loop_top, loop_bottom = [], []\n",
    "        loop_top.append(raw_spec[q_len : q_len + half_len])\n",
    "        loop_top.append(raw_spec[q_len + 2*half_len : q_len + 3*half_len])\n",
    "        loop_top.append(raw_spec[q_len + 4*half_len : 2*q_len + 4*half_len])\n",
    "        loop_bottom.append(raw_spec[:q_len])\n",
    "        loop_bottom.append(raw_spec[q_len + half_len: q_len + 2*half_len])\n",
    "        loop_bottom.append(raw_spec[q_len + 3*half_len: q_len + 4*half_len])\n",
    "        loop_top = np.concatenate(loop_top)\n",
    "        loop_bottom = np.concatenate(loop_bottom)\n",
    "        return np.abs(np.sum(loop_top)-np.sum(loop_bottom))\n",
    "\n",
    "    score = loop_area(raw_spec = spectrum, cycle = 3)# TODO: hard coded cycle now\n",
    "\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def black_box_positive_nucleation_bias(index, indices_all, e1a, e1b) -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score simulates the blackbox function\n",
    "    \"\"\"\n",
    "\n",
    "    e_start,e_end = e1a, e1b\n",
    "    array_list, shape, dtype = mic_server.get_point_data(\n",
    "        spectrum_image_index=\"Channel_001\",\n",
    "        x=int(indices_all[index, 0]),######### TODO: check if x anf y needs to be flipped\n",
    "        y=int(indices_all[index, 1])\n",
    "    )\n",
    "    spectrum = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "\n",
    "    def positive_nucleation_bias(raw_spec, cycle):\n",
    "        raw_spec_len = len(raw_spec)\n",
    "        cycle_len = int(raw_spec_len / cycle)\n",
    "        half_len = int(cycle_len / 2)\n",
    "        q_len = int(cycle_len / 4)\n",
    "        loop_top, loop_bottom = [], []\n",
    "        loop_top.append(raw_spec[q_len : q_len + half_len])\n",
    "        loop_top.append(raw_spec[q_len + 2*half_len : q_len + 3*half_len])\n",
    "        loop_top.append(raw_spec[q_len + 4*half_len : 2*q_len + 4*half_len])\n",
    "        loop_bottom.append(raw_spec[:q_len])\n",
    "        loop_bottom.append(raw_spec[q_len + half_len: q_len + 2*half_len])\n",
    "        loop_bottom.append(raw_spec[q_len + 3*half_len: q_len + 4*half_len])\n",
    "        loop_top = np.concatenate(loop_top)\n",
    "        loop_bottom = np.concatenate(loop_bottom)\n",
    "        return np.mean(loop_top) - np.mean(loop_bottom)\n",
    "\n",
    "    score = positive_nucleation_bias(raw_spec = spectrum, cycle = 3)# TODO: hard coded cycle now\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "def black_box_negative_nucleation_bias(index, indices_all, e1a, e1b) -> float:\n",
    "    \"\"\"\n",
    "    Black box function that returns a target score simulates the blackbox function\n",
    "    \"\"\"\n",
    "\n",
    "    e_start,e_end = e1a, e1b\n",
    "    array_list, shape, dtype = mic_server.get_point_data(\n",
    "        spectrum_image_index=\"Channel_001\",\n",
    "        x=int(indices_all[index, 0]),######### TODO: check if x anf y needs to be flipped\n",
    "        y=int(indices_all[index, 1])\n",
    "    )\n",
    "    spectrum = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "\n",
    "    def negative_nucleation_bias(raw_spec, cycle):\n",
    "        raw_spec_len = len(raw_spec)\n",
    "        cycle_len = int(raw_spec_len / cycle)\n",
    "        half_len = int(cycle_len / 2)\n",
    "        q_len = int(cycle_len / 4)\n",
    "        loop_top, loop_bottom = [], []\n",
    "        loop_top.append(raw_spec[q_len : q_len + half_len])\n",
    "        loop_top.append(raw_spec[q_len + 2*half_len : q_len + 3*half_len])\n",
    "        loop_top.append(raw_spec[q_len + 4*half_len : 2*q_len + 4*half_len])\n",
    "        loop_bottom.append(raw_spec[:q_len])\n",
    "        loop_bottom.append(raw_spec[q_len + half_len: q_len + 2*half_len])\n",
    "        loop_bottom.append(raw_spec[q_len + 3*half_len: q_len + 4*half_len])\n",
    "        loop_top = np.concatenate(loop_top)\n",
    "        loop_bottom = np.concatenate(loop_bottom)\n",
    "        return np.min(loop_top) - np.max(loop_bottom)\n",
    "\n",
    "    score = negative_nucleation_bias(raw_spec = spectrum, cycle = 3)# TODO: hard coded cycle now\n",
    "\n",
    "\n",
    "\n",
    "    return score\n",
    "\n",
    "## a) evaluations metrics like nlpd, mse ----\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Squared Error (MSE)\"\"\"\n",
    "    #Smaller values indicate better predictions.\n",
    "    #Squaring ensures that positive and negative errors don't cancel out.\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return mse\n",
    "\n",
    "def calculate_nlpd(y_true, y_pred_mean, y_pred_var):\n",
    "    \"\"\"Calculate Negative Log Predictive Density (NLPD)\"\"\"\n",
    "    #NLPD evaluates how well the predicted probability distribution matches the true values.\n",
    "    #Lower NLPD indicates a better match, accounting for both the mean and uncertainty.\n",
    "    nlpd = 0.5 * torch.log(2 * torch.pi * y_pred_var) + 0.5 * ((y_true - y_pred_mean) ** 2 / y_pred_var)\n",
    "    return nlpd.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Utility F:n's - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1735229227847,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "gQq7ZyWunB6D"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_scores_for_patches(unacquired_indices, indices_all, e1a, e1b, black_box_fn = black_box) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the score for each patch using the black_box function.\n",
    "\n",
    "    Parameters:\n",
    "    - patches: Tensor of all data patches.\n",
    "\n",
    "    Returns:\n",
    "    - scores: List of scores for each patch.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i in unacquired_indices:\n",
    "        score = black_box_fn(i, indices_all, e1a, e1b)  # Calculate score for each patch\n",
    "        scores.append(score)\n",
    "    return torch.tensor(scores)  # Return as a tensor for compatibility\n",
    "\n",
    "def update_acquired(acquired_data, unacquired_indices, selected_indices, indices_all, e1a, e1b, black_box_fn = black_box) -> (np.array, list):\n",
    "    for idx in selected_indices:# TODO: It queries the black box everytime on already acquired points:\n",
    "        acquired_data[idx] = black_box_fn(idx, indices_all, e1a, e1b)\n",
    "    unacquired_indices = [idx for idx in unacquired_indices if idx not in selected_indices]\n",
    "\n",
    "\n",
    "    return acquired_data, unacquired_indices\n",
    "\n",
    "\n",
    "def load_image_and_features(img: np.ndarray , window_size : int) -> (np.ndarray, np.ndarray):\n",
    "    coordinates = get_coord_grid(img, step=1, return_dict=False)\n",
    "    features_all, coords, _ = extract_subimages(img, coordinates, window_size)\n",
    "    features_all = features_all[:, :, :, 0]\n",
    "    coords = np.array(coords, dtype=int)\n",
    "    norm_ = lambda x: (x - x.min()) / x.ptp()\n",
    "    features = norm_(features_all)\n",
    "    return features, coords# shapes (3366, 5, 5) and (3366, 2)\n",
    "\n",
    "\n",
    "def prepare_data_from_microscope(window_size: int) -> (np.ndarray, np.ndarray):\n",
    "    array_list, shape, dtype = mic_server.get_overview_image()\n",
    "    img = np.array(array_list, dtype=dtype).reshape(shape)#\n",
    "    features, indices_all = load_image_and_features(img, window_size)\n",
    "\n",
    "\n",
    "\n",
    "    return img, features, indices_all# shapes (55, 70), (3366, 5, 5) and (3366, 2)\n",
    "\n",
    "def get_spectrum_data(indices, energy_range, channel=\"Channel_001\") -> (np.array, int, int):\n",
    "    array_list, shape, dtype = mic_server.get_spectrum_image(spectrum_image_index=channel)\n",
    "    spectral_img = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "    array_list, shape, dtype = mic_server.get_spectrum_image_e_axis(spectrum_image_index=channel)\n",
    "    E_axis = np.array(array_list, dtype=dtype).reshape(shape)\n",
    "    e_start, e_end = abs(E_axis - energy_range[0]).argmin(), abs(E_axis - energy_range[1]).argmin()\n",
    "    return spectral_img, e_start, e_end\n",
    "\n",
    "\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "\n",
    "def plot_pareto_front(acquired_data1, acquired_data2, step, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot the current Pareto front based on acquired data from both objectives and return Pareto indices.\n",
    "    \n",
    "    Args:\n",
    "        acquired_data1 (dict): Acquired data for Objective 1. Format: {index: value1}\n",
    "        acquired_data2 (dict): Acquired data for Objective 2. Format: {index: value2}\n",
    "        step (int): Current BO step.\n",
    "        save_path (str, optional): Path to save the plot. If None, displays the plot.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of indices that are Pareto optimal.\n",
    "    \"\"\"\n",
    "    if not acquired_data1 or not acquired_data2:\n",
    "        print(\"Insufficient data to plot Pareto front.\")\n",
    "        return []\n",
    "    \n",
    "    # Ensure both acquired_data1 and acquired_data2 have the same indices\n",
    "    common_indices = list(set(acquired_data1.keys()).intersection(set(acquired_data2.keys())))\n",
    "    if not common_indices:\n",
    "        print(\"No common indices between acquired_data1 and acquired_data2 for Pareto front plotting.\")\n",
    "        return []\n",
    "    \n",
    "    # Extract objective values\n",
    "    obj1 = np.array([acquired_data1[idx] for idx in common_indices])\n",
    "    obj2 = np.array([acquired_data2[idx] for idx in common_indices])\n",
    "    \n",
    "    obj1= (obj1 - obj1.min()) / (obj1.max() - obj1.min())######## normalized\n",
    "    obj2= (obj2 - obj2.min()) / (obj2.max() - obj2.min())######## normalized\n",
    "\n",
    "    \n",
    "    # Combine objectives into a single array\n",
    "    objectives = np.stack([obj1, obj2], axis=1)\n",
    "    objectives_tensor = torch.tensor(objectives, dtype=torch.float32)\n",
    "    \n",
    "    # Determine Pareto optimal points using BoTorch's is_non_dominated\n",
    "    pareto_mask = is_non_dominated(objectives_tensor)\n",
    "    \n",
    "    # Extract Pareto optimal points\n",
    "    pareto_obj1 = obj1[pareto_mask.numpy()]\n",
    "    pareto_obj2 = obj2[pareto_mask.numpy()]\n",
    "    \n",
    "    # Extract Pareto optimal indices\n",
    "    pareto_indices = [common_indices[i] for i, is_pareto in enumerate(pareto_mask.numpy()) if is_pareto]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(obj1, obj2, label='Acquired Points', color='blue')\n",
    "    plt.scatter(pareto_obj1, pareto_obj2, label='Pareto Front', color='red')\n",
    "    \n",
    "    # Optionally, annotate Pareto points with their indices\n",
    "    for idx, x, y in zip(pareto_indices, pareto_obj1, pareto_obj2):\n",
    "        plt.annotate(str(idx), (x, y), textcoords=\"offset points\", xytext=(5,5), ha='left', fontsize=8)\n",
    "    \n",
    "    plt.xlabel('Objective 1')\n",
    "    plt.ylabel('Objective 2')\n",
    "    plt.title(f'Pareto Front after BO Step {step +1}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    return pareto_indices\n",
    "\n",
    "\n",
    "def embeddings_and_predictions(model, patches, device=\"cpu\") -> (torch.Tensor, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Get predictions from the trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    patches = patches.to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(patches)\n",
    "        embeddings = model.feature_extractor(patches).view(patches.size(0), -1).cpu().numpy()\n",
    "    return predictions, embeddings\n",
    "\n",
    "def train_model(acquired_data, patches, feature_extractor,\n",
    "                device=\"cpu\", num_epochs=50, log_interval=5,\n",
    "                scalarizer_zero=False) -> ApproximateGP:\n",
    "    X_train = torch.stack([patches[idx] for idx in acquired_data]).to(device)\n",
    "    y_train = torch.tensor(list(acquired_data.values()), dtype=torch.float32).to(device)\n",
    "    if scalarizer_zero:\n",
    "        y_train = torch.zeros_like(y_train)\n",
    "\n",
    "    else:\n",
    "        # Normalize y_train\n",
    "\n",
    "        y_train = (y_train - y_train.min()) / (y_train.max() - y_train.min())\n",
    "\n",
    "\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "    inducing_points = X_train[:10]\n",
    "    model = GPModelDKL(inducing_points=inducing_points, likelihood=likelihood, feature_extractor=feature_extractor).to(device)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs + 1), desc=\"Training Progress\"):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train)\n",
    "\n",
    "\n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Bayesian optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1735229839511,
     "user": {
      "displayName": "Utkarsh Pratiush",
      "userId": "14303792046592850693"
     },
     "user_tz": -330
    },
    "id": "YHfE0zAJnB6D"
   },
   "outputs": [],
   "source": [
    "def run(config) -> None:\n",
    "    # Extract all configuration variables\n",
    "    seed = config[\"seed\"]\n",
    "    seed_pts = config[\"seed_pts\"]\n",
    "    budget = config[\"budget\"]\n",
    "    in_dir = config[\"in_dir\"]\n",
    "    out_dir_parent = config[\"out_dir_parent\"]\n",
    "    dataset_name = config[\"dataset_name\"]\n",
    "    device = config[\"device\"]\n",
    "    # initial_batch_size = config[\"initial_batch_size\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    normalize_data_flag = config[\"normalize_data\"]\n",
    "    window_size = config[\"window_size\"]\n",
    "    scal_pfm1 = config[\"scal_pfm1\"]\n",
    "    scal_pfm2 = config[\"scal_pfm2\"]\n",
    "    ## need scal1 and scal2\n",
    "\n",
    "    energy_range1 = [0, 1]# TODO: can be confusing as used in eels data - for now ignore for beps\n",
    "    energy_range2 = [0, 1]# TODO: can be confusing as used in eels data - for now ignore for beps\n",
    "\n",
    "    if scal_pfm1 is not None:## only for pfm: TODO : find better to accomodiate this\n",
    "        if scal_pfm1 == \"loop_area\":\n",
    "            black_box_fn1 = black_box_loop_height\n",
    "        \n",
    "        elif scal_pfm1 == \"loop_height\":\n",
    "            black_box_fn1 = black_box_loop_height\n",
    "            \n",
    "        elif scal_pfm1 == \"positive_nucleation_bias\":\n",
    "            black_box_fn1 = black_box_positive_nucleation_bias\n",
    "\n",
    "        elif scal_pfm1 ==  \"negative_nucleation_bias\":\n",
    "            black_box_fn1 = black_box_negative_nucleation_bias\n",
    "        \n",
    "    else :\n",
    "        black_box_fn1 = black_box\n",
    "\n",
    "    if scal_pfm2 is not None:## only for pfm: TODO : find better to accomodiate this\n",
    "        if scal_pfm2 == \"loop_area\":\n",
    "            black_box_fn2 = black_box_loop_height\n",
    "        \n",
    "        elif scal_pfm2 == \"loop_height\":\n",
    "            black_box_fn2 = black_box_loop_height\n",
    "            \n",
    "        elif scal_pfm2 == \"positive_nucleation_bias\":\n",
    "            black_box_fn2 = black_box_positive_nucleation_bias\n",
    "\n",
    "        elif scal_pfm2 ==  \"negative_nucleation_bias\":\n",
    "            black_box_fn2 = black_box_negative_nucleation_bias\n",
    "        \n",
    "    else :\n",
    "        black_box_fn2 = black_box\n",
    "    \n",
    "    scalarizer_zero = False # TODO: deafult value to zero -- so passed to train_model function --> better way to handel\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    res_dir = Path(out_dir_parent) / f\"benchmark/MOBO_pareto_hack_Dataset_seed{seed}_{dataset_name}_BO_{seed_pts}_epochs{num_epochs}_budget_{budget}_{scal_pfm1}_{scal_pfm2}_ws{window_size}_{timestamp}\"\n",
    "    res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Connect to the microscope server\n",
    "    uri = \"PYRO:microscope.server@localhost:9091\"\n",
    "    global mic_server # TODO: later see better way to do this\n",
    "    mic_server = Pyro5.api.Proxy(uri)\n",
    "\n",
    "    dataset_path = in_dir + \"/\" +  dataset_name\n",
    "    ### 2. Download data and register\n",
    "    # !wget https://github.com/pycroscopy/DTMicroscope/raw/utk/data/STEM/SI/test_stem.h5\n",
    "    mic_server.initialize_microscope(\"STEM\")\n",
    "    mic_server.register_data(dataset_path)\n",
    "\n",
    "\n",
    "    # Prepare features and indices from microscope image\n",
    "    img, features, indices_all = prepare_data_from_microscope(window_size=window_size)\n",
    "    ############################################\n",
    "    \n",
    "    \n",
    "    patches = numpy_to_torch_for_conv(features)\n",
    "\n",
    "    # Set up energy ranges for scalarizer extraction\n",
    "    spectral_img1, e1a, e1b = get_spectrum_data(indices_all, energy_range1)\n",
    "    spectral_img2, e2a, e2b = get_spectrum_data(indices_all, energy_range2)\n",
    "\n",
    "\n",
    "\n",
    "    patches = patches.to(device)\n",
    "\n",
    "    if normalize_data_flag:\n",
    "        patches = normalize_data(patches)\n",
    "\n",
    "    feature_extractor1 = ConvNetFeatureExtractor(input_channels=1, output_dim=2).to(device)\n",
    "    feature_extractor2 = ConvNetFeatureExtractor(input_channels=1, output_dim=2).to(device)\n",
    "\n",
    "    acquired_data1 = {}\n",
    "    acquired_data2 = {}\n",
    "    unacquired_indices1 = list(range(len(indices_all)))####### TODO: need to change later to use the indices_all\n",
    "    unacquired_indices2 = list(range(len(indices_all)))####### TODO: need to change later to use the indices_all\n",
    "\n",
    "    selected_indices1 = random.sample(unacquired_indices1, seed_pts)\n",
    "    selected_indices2 = selected_indices1\n",
    "     \n",
    "    seed_indices = selected_indices1\n",
    "    ######### queries spectrum_image\n",
    "    true_scalarizer1= calculate_scores_for_patches(unacquired_indices1, indices_all, e1a, e1b, black_box_fn=black_box_fn1)\n",
    "    true_scalarizer1= (true_scalarizer1 - true_scalarizer1.min()) / (true_scalarizer1.max() - true_scalarizer1.min())######## normalized\n",
    "    true_scalarizer2= calculate_scores_for_patches(unacquired_indices2, indices_all, e2a, e2b, black_box_fn=black_box_fn2)\n",
    "    true_scalarizer2= (true_scalarizer2 - true_scalarizer2.min()) / (true_scalarizer2.max() - true_scalarizer2.min())######## normalized\n",
    "\n",
    "        \n",
    "    ######### queries microscope \n",
    "    acquired_data2, unacquired_indices2 = update_acquired(acquired_data2, unacquired_indices2, selected_indices2, indices_all, e2a, e2b, black_box_fn= black_box_fn2)\n",
    "    acquired_data1, unacquired_indices1 = update_acquired(acquired_data1, unacquired_indices1, selected_indices1, indices_all, e1a, e1b, black_box_fn= black_box_fn1)\n",
    "\n",
    "    \n",
    "\n",
    "    from botorch.acquisition import LogExpectedImprovement #ExpectedImprovement\n",
    "    mean_y_pred_mean_al = []\n",
    "    mean_y_pred_variance_al = []\n",
    "    mae_list = []\n",
    "    nlpd_list = []\n",
    "    # Start Bayesian Optimization loop\n",
    "    for step in range(budget):\n",
    "\n",
    "\n",
    "        # Train the DKL model\n",
    "        model1 = train_model(acquired_data1, patches, feature_extractor1, device=device, num_epochs=num_epochs, scalarizer_zero=scalarizer_zero)\n",
    "        model2 = train_model(acquired_data2, patches, feature_extractor2, device=device, num_epochs=num_epochs, scalarizer_zero=scalarizer_zero)\n",
    "\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "\n",
    "\n",
    "        # Wrap the model and likelihood in the BoTorch model ------> Ithink not needed as have approxiamateGP--> check later\n",
    "\n",
    "        # Prepare candidate set (unacquired patches)\n",
    "        candidate_indices = unacquired_indices1\n",
    "        X_candidates = torch.stack([patches[idx] for idx in candidate_indices]).to(device)\n",
    "        # X_candidates = X_candidates.reshape(-1, window_size*window_size)# TODO: to make botorch acquisition functions compatible-> optimize_acqf_discrete\n",
    "        X_candidates = X_candidates.reshape(-1, 1, window_size*window_size) # Note this is when using acq f:n directly and not invoking  optimize_acqf_discrete\n",
    "        # as we are normalizing the y_values in the train_model function:y_train = (y_train - y_train.min()) / (y_train.max() - y_train.min())\n",
    "        # TODO: make it better -> Note we dont normalize entire data as we dont have access to entire spectrum image early on --> so just on seed points\n",
    "        y_train1 = torch.tensor(list(acquired_data1.values()), dtype=torch.float32).to(device)\n",
    "        y_train1 = (y_train1 - y_train1.min()) / (y_train1.max() - y_train1.min())\n",
    "    \n",
    "        y_train2 = torch.tensor(list(acquired_data2.values()), dtype=torch.float32).to(device)\n",
    "        y_train2 = (y_train2 - y_train2.min()) / (y_train2.max() - y_train2.min())\n",
    "\n",
    "        acq_func1 = LogExpectedImprovement(model=model1, best_f=y_train1.max().to(device))\n",
    "        acq_func2 = LogExpectedImprovement(model=model2, best_f=y_train2.max().to(device))\n",
    "\n",
    "\n",
    "        acq_values1 = acq_func1(X_candidates)\n",
    "        acq_values2 = acq_func2(X_candidates)\n",
    "        \n",
    "        # Stack acquisition values into a single tensor of shape (Q, 2)\n",
    "        acq_matrix = torch.stack([acq_values1, acq_values2], dim=1)\n",
    "\n",
    "        # Determine Pareto optimal points\n",
    "        pareto_mask = is_non_dominated(acq_matrix)\n",
    "\n",
    "        # Convert mask to boolean numpy array\n",
    "        pareto_mask_np = pareto_mask.cpu().numpy()\n",
    "\n",
    "        # Extract Pareto optimal indices\n",
    "        pareto_indices = np.where(pareto_mask_np)[0]\n",
    "\n",
    "\n",
    "        # Among Pareto optimal points, compute aggregate acquisition value and select the highest\n",
    "        if len(pareto_indices) == 0:\n",
    "            # If no Pareto optimal points, fallback to selecting the point with the highest acq_1\n",
    "            selected_index = torch.argmax(acq_values1).item()\n",
    "        else:\n",
    "            # Extract acquisition values for Pareto optimal points\n",
    "            pareto_acq1 = acq_values1[pareto_mask]\n",
    "            pareto_acq2 = acq_values2[pareto_mask]\n",
    "            \n",
    "            # Compute aggregate acquisition value (e.g., sum or average)\n",
    "            aggregate_acq = pareto_acq1 + pareto_acq2  # Simple sum; can also use weighted sum or average\n",
    "            \n",
    "            # Select the Pareto point with the highest aggregate acquisition value\n",
    "            selected_pareto_idx = torch.argmax(aggregate_acq).item()\n",
    "            selected_index = pareto_indices[selected_pareto_idx]\n",
    "\n",
    "        selected_indices = [selected_index]#### can be multiple indices if batch acquisition\n",
    "\n",
    "        # Update acquired data with new observations\n",
    "        acquired_data1, unacquired_indices1 = update_acquired(acquired_data1, unacquired_indices1, selected_indices, indices_all, e1a, e1b)\n",
    "        acquired_data2, unacquired_indices2 = update_acquired(acquired_data2, unacquired_indices2, selected_indices, indices_all, e2a, e2b)\n",
    "\n",
    "\n",
    "        pareto_indices = plot_pareto_front(acquired_data1, acquired_data2, step, save_path=f\"{res_dir}/Pareto_plot_step{step}.png\")\n",
    "\n",
    "        print(f\"**************************done BO step {step +1}\")\n",
    "        print(\"total points in pareto_indices\",len(pareto_indices))\n",
    "        \n",
    "\n",
    "    # Save predictions as a .pkl file\n",
    "    Active_learning_statistics = {\n",
    "        \"img\": img,\n",
    "        \"features\": features,\n",
    "        \"indices_all\": np.array(indices_all),\n",
    "        \"seed_indices\": np.array(seed_indices),\n",
    "        \"unacquired_indices\": np.array(unacquired_indices1),\n",
    "        \"mean_y_pred_mean_al\": np.array(mean_y_pred_mean_al),\n",
    "        \"mean_y_pred_variance_al\": np.array(mean_y_pred_variance_al)\n",
    "        # \"mae\": np.array(mae_list),\n",
    "        # \"nlpd\": np.array(nlpd_list)\n",
    "                }\n",
    "\n",
    "    with open(Path(res_dir) / f'Active_learning_statistics.pkl', 'wb') as f:\n",
    "        pickle.dump(Active_learning_statistics, f)\n",
    "\n",
    "  \n",
    "    ##############################-> plot for group meetin\n",
    "\n",
    "    predictions_data = Active_learning_statistics\n",
    "    # Extract necessary data\n",
    "    img = np.array(predictions_data[\"img\"])  # Image or grid for background visualization\n",
    "    seed_indices = np.array(predictions_data[\"seed_indices\"])  # Initial sampled indices (referring to positions in indices_all)\n",
    "    unacquired_indices = np.array(predictions_data[\"unacquired_indices\"])  # Remaining indices\n",
    "    indices_all = np.array(predictions_data[\"indices_all\"])  # All possible indices (coordinates)\n",
    "\n",
    "    # Map seed_indices and unacquired_indices to their coordinates in indices_all\n",
    "    seed_coords = indices_all[seed_indices]\n",
    "    unacquired_coords = indices_all[unacquired_indices]\n",
    "\n",
    "    # Calculate acquired indices as the complement of unacquired and seed indices\n",
    "    acquired_indices = np.setdiff1d(np.arange(indices_all.shape[0]), np.union1d(seed_indices, unacquired_indices), assume_unique=True)\n",
    "    acquired_coords = indices_all[acquired_indices]\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Display the image or grid as the background\n",
    "    plt.imshow(img, cmap=\"gray\", origin=\"lower\")\n",
    "\n",
    "    # Plot the seed points in blue\n",
    "    plt.scatter(seed_coords[:, 1], seed_coords[:, 0], c=\"b\", label=\"Seed Points\", marker=\"o\")\n",
    "\n",
    "    time_order = np.arange(len(acquired_coords))  # Create a sequence representing time\n",
    "    scatter = plt.scatter(acquired_coords[:, 1], acquired_coords[:, 0], c=time_order, cmap=\"bwr\", label=\"Acquired Points\", marker=\"x\")\n",
    "\n",
    "    # Plot the unacquired points in green\n",
    "    # plt.scatter(unacquired_coords[:, 1], unacquired_coords[:, 0], c=\"g\", label=\"Unacquired Points\", marker=\"+\")\n",
    "\n",
    "    # Set plot labels and legend\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "    plt.title(\"Active Learning Trajectory\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    # Add a colorbar and label it as \"Steps\"\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label(\"Steps\")\n",
    "\n",
    "\n",
    "    plt.savefig(Path(res_dir) / \"AL_traj.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3f. Set parameters and Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0f3-LDlSnB6E",
    "outputId": "9fca0bc5-75a2-41f3-f777-98dfa1fbfe09"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"seed\" : 1, # for repeatibility\n",
    "        \"seed_pts\" : 10, # How many points you want to start your BO with?\n",
    "        \"budget\" : 50, # How many experimental budget you have?\n",
    "        \"in_dir\": \".\", # recommended : leave as is\n",
    "        \"out_dir_parent\": \"out\", # recommended : leave as is\n",
    "        \"dataset_name\": \"yl_beps.h5\", # name of data to be loaded in DTmicroscope\n",
    "        \"device\": \"cuda\",\n",
    "        \"num_epochs\": 100, # Number of epoch the dkl model trains at each experimental step - Might need tuning based on data\n",
    "        \"normalize_data\": True, \n",
    "        \"window_size\": 16, # For square patches - structure property relationship\n",
    "        \"scal_pfm1\": \"positive_nucleation_bias\", # What physics interested in? options on this data: \"loop_area\", \"loop_height\", \"positive_nucleation_bias\", \"negative_nucleation_bias\",\n",
    "        \"scal_pfm1\": \"negative_nucleation_bias\", # What physics interested in? options on this data: \"loop_area\", \"loop_height\", \"positive_nucleation_bias\", \"negative_nucleation_bias\"\n",
    "        }\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "qBO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
