{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfb8d16",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/utkarshp1161/Active-learning-in-microscopy/blob/main/notebooks/Beacon-combi-library.ipynb)\n",
    "# Notebook illustrating novelty search - BEACON method - (2dim input space) to (1 or 2 dim target space)\n",
    "\n",
    "## Here we apply it to combi library exploration on the grid data - We benchmark the exploration on various acquisiton functions.\n",
    "- credits\n",
    "    - original paper - [BEACON: A Bayesian Optimization Strategy for Novelty Search in Expensive Black-Box Systems](https://arxiv.org/abs/2406.03616)\n",
    "        - Wei-Ting Tang, Ankush Chakrabarty, Joel A. Paulson\n",
    "    - Adapted code by [Utkarsh Pratiush](https://github.com/utkarshp1161)\n",
    "\n",
    "    - Data by [Richard Liu](https://github.com/RichardLiuCoding)\n",
    "\n",
    "# Note: Recommended to take a GPU instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd08d0",
   "metadata": {},
   "source": [
    "## 1. Install stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d176b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# botorch - gpytorch etc\n",
    "#install\n",
    "!pip install botorch==0.12.0\n",
    "!pip install gpytorch==1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2849bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from botorch.models import SingleTaskGP, ModelListGP\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from botorch.acquisition import AcquisitionFunction\n",
    "from botorch.utils.transforms import t_batch_mode_transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.models.model import Model\n",
    "from typing import Optional\n",
    "from botorch.acquisition.objective import PosteriorTransform\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0570bd9",
   "metadata": {},
   "source": [
    "## 2. Write the Thompson sampler which is core for sampling in beacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e182f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thompson sampling on GPU - credits - REPO: https://github.com/PaulsonLab/BEACON\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from math import pi\n",
    "from typing import Any, Tuple\n",
    "from botorch.models.model import Model\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.posteriors import Posterior\n",
    "from botorch.models.transforms import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "class EfficientThompsonSampler():\n",
    "    def __init__(self, model, num_of_multistarts=1, num_of_bases=1024, num_of_samples=1):\n",
    "        '''\n",
    "        Implementation of 'Efficiently Sampling From Gaussian Process Posteriors' by Wilson et al. (2020). It allows\n",
    "        us to create approximate samples of the GP posterior, which we can optimise using gradient methods. We do this\n",
    "        to generate candidates using Thompson Sampling. Link to the paper: https://arxiv.org/pdf/2002.09309.pdf .\n",
    "        \n",
    "        GPU-enabled version.\n",
    "        '''\n",
    "        # GP model\n",
    "        self.model = model\n",
    "        \n",
    "        # Determine device from model\n",
    "        if hasattr(model, 'train_inputs'):\n",
    "            if isinstance(model.train_inputs, tuple):\n",
    "                self.device = model.train_inputs[0].device\n",
    "            else:\n",
    "                self.device = model.train_inputs.device\n",
    "        else:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # inputs\n",
    "        if type(self.model.train_x) == torch.Tensor:\n",
    "            self.train_x = self.model.train_x.to(self.device)\n",
    "        else:\n",
    "            self.train_x = torch.tensor(self.model.train_x, device=self.device)\n",
    "        \n",
    "        self.x_dim = torch.tensor(self.train_x.shape[1], device=self.device)\n",
    "        self.train_y = self.model.train_y.to(self.device)\n",
    "        self.num_of_train_inputs = self.model.train_x.shape[0]\n",
    "        \n",
    "        # scaled outputs\n",
    "        self.scaled_train_y = self.model.outcome_transform(self.train_y)[0].to(self.device)\n",
    "        \n",
    "        # thompson sampling parameters\n",
    "        self.num_of_multistarts = num_of_multistarts\n",
    "        self.num_of_bases = num_of_bases\n",
    "        self.num_of_samples = num_of_samples\n",
    "        \n",
    "        # optimisation parameters\n",
    "        self.learning_rate = 0.01\n",
    "        self.num_of_epochs = 10 * self.x_dim\n",
    "        \n",
    "        # obtain the kernel parameters\n",
    "        self.sigma = self.model.likelihood.noise[0].item()  # assumes fixed noise value\n",
    "        self.lengthscale = self.model.covar_module.base_kernel.lengthscale.detach().float().to(self.device)\n",
    "        self.outputscale = self.model.covar_module.outputscale.item()\n",
    "        \n",
    "        # obtain the kernel\n",
    "        self.kernel = self.model.covar_module\n",
    "        \n",
    "        # define the Knn matrix\n",
    "        with torch.no_grad():\n",
    "            self.Knn = self.kernel(self.train_x)\n",
    "            self.Knn = self.Knn.evaluate().to(self.device)\n",
    "            # precalculate matrix inverse\n",
    "            self.inv_mat = torch.inverse(\n",
    "                self.Knn + self.sigma * torch.eye(self.num_of_train_inputs, device=self.device)\n",
    "            )\n",
    "\n",
    "        self.create_fourier_bases()\n",
    "        self.calculate_phi()\n",
    "\n",
    "    def create_fourier_bases(self):\n",
    "        # sample thetas\n",
    "        self.thetas = torch.randn(\n",
    "            size=(self.num_of_bases, self.x_dim), \n",
    "            device=self.device\n",
    "        ) / self.lengthscale\n",
    "        # sample biases\n",
    "        self.biases = torch.rand(self.num_of_bases, device=self.device) * 2 * pi\n",
    "\n",
    "    def create_sample(self):\n",
    "        # sample weights\n",
    "        self.weights = torch.randn(\n",
    "            size=(self.num_of_samples, self.num_of_bases), \n",
    "            device=self.device\n",
    "        ).float()\n",
    "\n",
    "    def calculate_phi(self):\n",
    "        '''\n",
    "        From the paper, we are required to calculate a matrix which includes the evaluation of the training set, X_train,\n",
    "        at the fourier frequencies. This function calculates that matrix, Phi.\n",
    "        '''\n",
    "        # we take the dot product by element-wise multiplication followed by summation\n",
    "        thetas = self.thetas.repeat(self.num_of_train_inputs, 1, 1)\n",
    "        prod = thetas * self.train_x.unsqueeze(1)\n",
    "        dot = torch.sum(prod, axis=-1)\n",
    "        # add biases and take cosine to obtain fourier representations\n",
    "        ft = torch.cos(dot + self.biases.unsqueeze(0))\n",
    "        # finally, multiply by corresponding constants (see paper)\n",
    "        self.Phi = (self.outputscale * np.sqrt(2 / self.num_of_bases) * ft).float()\n",
    "\n",
    "    def calculate_V(self):\n",
    "        '''\n",
    "        From the paper, to give posterior updates we need to calculate the vector V. Since we are doing multiple samples\n",
    "        at the same time, V will be a matrix. We can pre-calculate it, since its value does not depend on the query locations.\n",
    "        '''\n",
    "        # multiply phi matrix by weights\n",
    "        # PhiW: num_of_train x num_of_samples\n",
    "        PhiW = torch.matmul(self.Phi, self.weights.T)\n",
    "        # add noise (see paper)\n",
    "        PhiW = PhiW + torch.randn(size=PhiW.shape, device=self.device) * self.sigma\n",
    "        # subtract from training outputs\n",
    "        mat1 = self.scaled_train_y - PhiW\n",
    "        # calculate V matrix by premultiplication by inv_mat = (K_nn + I_n*sigma)^{-1}\n",
    "        # V: num_of_train x num_of_samples\n",
    "        self.V = torch.matmul(self.inv_mat, mat1)\n",
    "\n",
    "    def calculate_fourier_features(self, x):\n",
    "        '''\n",
    "        Calculate the Fourier Features evaluated at some input x\n",
    "        '''\n",
    "        # Make sure x is on the correct device\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device)\n",
    "        else:\n",
    "            x = x.to(self.device)\n",
    "        \n",
    "        # evaluation using fourier features\n",
    "        self.posterior_update(x)\n",
    "        # calculate the dot product between the frequencies, theta, and the new query points\n",
    "        dot = x.matmul(self.thetas.T)\n",
    "        # calculate the fourier frequency by adding bias and cosine\n",
    "        ft = torch.cos(dot + self.biases.unsqueeze(0))\n",
    "        # apply the normalising constants and return the output\n",
    "        return self.outputscale * np.sqrt(2 / self.num_of_bases) * ft\n",
    "\n",
    "    def sample_prior(self, x):\n",
    "        '''\n",
    "        Create a sample form the prior, evaluate it at x\n",
    "        '''\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device)\n",
    "        else:\n",
    "            x = x.to(self.device)\n",
    "        \n",
    "        # calculate the fourier features evaluated at the query points\n",
    "        out1 = self.calculate_fourier_features(x)\n",
    "        # extend the weights so that we can use element wise multiplication\n",
    "        weights = self.weights.repeat(self.num_of_multistarts, 1, 1)\n",
    "        # return the prior\n",
    "        return torch.sum(weights * out1, axis=-1)\n",
    "\n",
    "    def posterior_update(self, x):\n",
    "        '''\n",
    "        Calculate the posterior update at a location x\n",
    "        '''\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device)\n",
    "        else:\n",
    "            x = x.to(self.device)\n",
    "        \n",
    "        # x: num_of_multistarts x num_of_samples x dim\n",
    "        self.calculate_V()  # can probably pre-calculate this\n",
    "        # train x: num_of_multistarts x num_of_train x dim\n",
    "        train_x = self.train_x.repeat(self.num_of_multistarts, 1, 1)\n",
    "        # z: num_of_multistarts x num_of_train x num_of_samples\n",
    "        # z: kernel evaluation between new query points and training set\n",
    "        z = self.kernel(train_x, x)\n",
    "        z = z.evaluate()\n",
    "        # we now repeat V the number of times necessary so that we can use element-wise multiplication\n",
    "        V = self.V.repeat(self.num_of_multistarts, 1, 1)\n",
    "        out = z * V\n",
    "        return out.sum(axis=1)  # we return the sum across the number of training point, as per the paper\n",
    "\n",
    "    def query_sample(self, x):\n",
    "        '''\n",
    "        Query the sample at a location\n",
    "        '''\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device)\n",
    "        else:\n",
    "            x = x.to(self.device)\n",
    "        \n",
    "        prior = self.sample_prior(x)\n",
    "        update = self.posterior_update(x)\n",
    "        y_scaled = prior + update\n",
    "        return self.model.outcome_transform.untransform(y_scaled)[0]\n",
    "\n",
    "    def generate_candidates(self):\n",
    "        '''\n",
    "        Generate the Thompson Samples, this function optimizes the samples.\n",
    "        '''\n",
    "        # we are always working on [0, 1]^d\n",
    "        bounds = torch.stack([\n",
    "            torch.zeros(self.x_dim, device=self.device), \n",
    "            torch.ones(self.x_dim, device=self.device)\n",
    "        ])\n",
    "        \n",
    "        # initialise randomly - there is definitely much better ways of doing this\n",
    "        X = torch.rand(\n",
    "            self.num_of_multistarts, \n",
    "            self.num_of_samples, \n",
    "            self.x_dim, \n",
    "            device=self.device\n",
    "        )\n",
    "        X.requires_grad = True\n",
    "        \n",
    "        # define optimiser\n",
    "        optimiser = torch.optim.Adam([X], lr=self.learning_rate)\n",
    "\n",
    "        for _ in range(self.num_of_epochs):\n",
    "            # set zero grad\n",
    "            optimiser.zero_grad()\n",
    "            # evaluate loss and backpropagate\n",
    "            losses = -self.query_sample(X)\n",
    "            loss = losses.sum()\n",
    "            loss.backward()\n",
    "            # take step\n",
    "            optimiser.step()\n",
    "\n",
    "            # make sure we are still within the bounds\n",
    "            for j, (lb, ub) in enumerate(zip(*bounds)):\n",
    "                X.data[..., j].clamp_(lb, ub)  # need to do this on the data not X itself\n",
    "        \n",
    "        # check the final evaluations\n",
    "        final_evals = self.query_sample(X)\n",
    "        # choose the best one for each sample\n",
    "        best_idx = torch.argmax(final_evals, axis=0)\n",
    "        # return the best one for each sample, without gradients\n",
    "        X_out = X[best_idx, range(0, self.num_of_samples), :]\n",
    "        return X_out.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a1bab",
   "metadata": {},
   "source": [
    "## 2. Download combi data and load it\n",
    "- Data credits - Richard Liu\n",
    "- Sample - AlScBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5749421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_disk(filepath, device='cpu'):\n",
    "    \"\"\"\n",
    "    Load data from disk.\n",
    "    Expected format: dictionary with 'X_measured' (70, 2) and 'y_measured' (70, 2+)\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    # data = torch.load(filepath, map_location=device)\n",
    "    \n",
    "    X = data[\"X_measured\"]  # (70, 2)\n",
    "    Y = data[\"y_measured\"][:, 2]  # (70, 2 or more)\n",
    "    \n",
    "    # Convert to tensors if not already\n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    if not isinstance(Y, torch.Tensor):\n",
    "        Y = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Ensure correct device and dtype\n",
    "    X = X.to(device).float()\n",
    "    Y = Y.to(device).float()\n",
    "    \n",
    "    # Handle 1D output case\n",
    "    if Y.dim() == 1:\n",
    "        Y = Y.unsqueeze(1)\n",
    "    \n",
    "    print(f\"Loaded X shape: {X.shape}\")\n",
    "    print(f\"Loaded Y shape: {Y.shape}\")\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def visualize_loaded_data(X, Y, save_path='loaded_data_visualization.png'):\n",
    "    \"\"\"Visualize the loaded data.\"\"\"\n",
    "    # Move to CPU for plotting\n",
    "    X_cpu = X.cpu().detach().numpy()\n",
    "    Y_cpu = Y.cpu().detach().numpy()\n",
    "    \n",
    "    n_outputs = Y.shape[1]\n",
    "    n_cols = min(3, n_outputs)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=(6*n_cols, 4))\n",
    "    \n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i in range(n_cols):\n",
    "        sc = axes[i].scatter(X_cpu[:, 0], X_cpu[:, 1], c=Y_cpu[:, i], \n",
    "                            cmap='viridis', s=60, edgecolors='black', linewidth=0.5)\n",
    "        axes[i].set_title(f'y_measured[{i}]', fontsize=12, fontweight='bold')\n",
    "        axes[i].set_xlabel('X position', fontsize=10, fontweight='bold')\n",
    "        axes[i].set_ylabel('Y position', fontsize=10, fontweight='bold')\n",
    "        fig.colorbar(sc, ax=axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Data visualization saved to '{save_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3dc55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## download pickle file - gdown--> data credits Richard Liu\n",
    "import pickle\n",
    "!gdown https://drive.google.com/uc?id=1SxcpoP-gX5k4FLtb8WjeMRzbmpc6EgK_\n",
    "pickle_path = \"AlScBN_processed_data.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7f744",
   "metadata": {},
   "source": [
    "## 3. Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b74aa",
   "metadata": {},
   "source": [
    "### 3a. Reachibility - defines notion of spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reachability_uniformity(behavior, n_bins=25, mins=None, maxs=None, num_filled_grids=100):\n",
    "    \"\"\"\n",
    "    Compute coverage metric based on grid filling in objective space.\n",
    "    Works for any number of output dimensions.\n",
    "    \"\"\"\n",
    "    filled_grids = set()\n",
    "    grid_sizes = (maxs - mins) / n_bins\n",
    "    output_dim = behavior.shape[1]\n",
    "    \n",
    "    for point in behavior:\n",
    "        # Handle boundary cases\n",
    "        grid_indices = torch.zeros(output_dim, dtype=torch.long, device=behavior.device)\n",
    "        for d in range(output_dim):\n",
    "            if point[d] >= maxs[d]:\n",
    "                grid_indices[d] = n_bins - 1\n",
    "            else:\n",
    "                grid_indices[d] = int((point[d] - mins[d]) / grid_sizes[d])\n",
    "        \n",
    "        filled_grids.add(tuple(grid_indices.cpu().tolist()))\n",
    "    \n",
    "    return len(filled_grids) / num_filled_grids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29d36a2",
   "metadata": {},
   "source": [
    "### 3b. Different acquisiton funcitons for comparison\n",
    "- Beacon\n",
    "- MaxVariance\n",
    "- Novelty search feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b000e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAcquisitionFunction(AcquisitionFunction):\n",
    "    \"\"\"BEACON: Novelty search acquisition function using k-NN distance in objective space.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, sampled_behavior, k_NN=10, output_dim=2):\n",
    "        super().__init__(model=model)\n",
    "        self.model = model\n",
    "        self.k_NN = k_NN\n",
    "        self.sampled_behavior = sampled_behavior\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Create Thompson samplers for all output dimensions\n",
    "        self.ts_samplers = []\n",
    "        for i in range(output_dim):\n",
    "            sampler = EfficientThompsonSampler(model.models[i])\n",
    "            sampler.create_sample()\n",
    "            self.ts_samplers.append(sampler)\n",
    "    \n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute acquisition function value at X.\"\"\"\n",
    "        # Collect samples from all output dimensions\n",
    "        samples_list = []\n",
    "        for sampler in self.ts_samplers:\n",
    "            samples_list.append(sampler.query_sample(X))\n",
    "        \n",
    "        samples = torch.cat(samples_list, dim=1)\n",
    "        \n",
    "        dist = torch.cdist(samples.to(torch.float64), self.sampled_behavior.to(torch.float64))\n",
    "        dist, _ = torch.sort(dist, dim=1)\n",
    "        \n",
    "        n = dist.size()[1]\n",
    "        E = torch.cat((torch.ones(self.k_NN, device=dist.device), \n",
    "                      torch.zeros(n - self.k_NN, device=dist.device)), dim=0)\n",
    "        dist = dist * E\n",
    "        acquisition_values = torch.sum(dist, dim=1)\n",
    "        \n",
    "        return acquisition_values.flatten()\n",
    "\n",
    "\n",
    "class MaxVariance(AcquisitionFunction):\n",
    "    \"\"\"MaxVar: Sum of posterior variances across objectives.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        output_dim: int = 2,\n",
    "        posterior_transform: Optional[PosteriorTransform] = None,\n",
    "        maximize: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__(model=model)\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    @t_batch_mode_transform(expected_q=1)\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        \"\"\"Evaluate the posterior variance on the candidate set X.\"\"\"\n",
    "        total_variance = torch.zeros(X.shape[0], device=X.device)\n",
    "        \n",
    "        for i in range(self.output_dim):\n",
    "            posterior = self.model.models[i].posterior(X=X)\n",
    "            variance = posterior.variance.clamp_min(1e-12)\n",
    "            total_variance += variance.flatten()\n",
    "        \n",
    "        return total_variance\n",
    "\n",
    "\n",
    "class NoveltySearchFeatureSpace():\n",
    "    \"\"\"NS-FS: Novelty search in feature (input) space using k-NN.\"\"\"\n",
    "    \n",
    "    def __init__(self, sampled_X, k=10):\n",
    "        self.k = k\n",
    "        self.sampled_X = sampled_X\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        \"\"\"Compute the acquisition function value at X.\"\"\"\n",
    "        dist = torch.cdist(X.to(torch.float64), self.sampled_X.to(torch.float64))\n",
    "        dist, _ = torch.sort(dist, dim=1)\n",
    "        n = dist.size()[1]\n",
    "        E = torch.cat((torch.ones(self.k, device=dist.device), \n",
    "                      torch.zeros(n - self.k, device=dist.device)), dim=0)\n",
    "        dist = dist * E\n",
    "        acquisition_values = torch.sum(dist, dim=1)\n",
    "        return acquisition_values.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac1cce",
   "metadata": {},
   "source": [
    "## 4. Experiment LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a7411",
   "metadata": {},
   "source": [
    "### 4a. run-beacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f015ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_beacon(X_original, Y_original, N_init, BO_iter, n_bins, mins, maxs, \n",
    "               num_filled_grids, k_NN, replicate, input_dim, output_dim, device):\n",
    "    \"\"\"Run BEACON (Novelty Search in Objective Space with Thompson Sampling).\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Running BEACON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cost_tensor = []\n",
    "    coverage_tensor = []\n",
    "    \n",
    "    n_total_points = len(X_original)\n",
    "    \n",
    "    for seed in range(replicate):\n",
    "        print(f'\\n=== BEACON Replicate {seed + 1}/{replicate} ===')\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        ids_acquired = np.random.choice(np.arange(n_total_points), size=N_init, replace=False)\n",
    "        train_x = X_original[ids_acquired].to(device)\n",
    "        train_y = Y_original[ids_acquired].to(device)\n",
    "        \n",
    "        coverage = reachability_uniformity(train_y, n_bins, mins, maxs, num_filled_grids)\n",
    "        coverage_list = [coverage]\n",
    "        cost_list = [0]\n",
    "        \n",
    "        for i in range(BO_iter):\n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"  Iteration {i + 1}/{BO_iter}, Coverage: {coverage:.4f}\")\n",
    "            \n",
    "            # Fit GPs for all output dimensions\n",
    "            model_list = []\n",
    "            for obj_idx in range(output_dim):\n",
    "                covar_module = ScaleKernel(RBFKernel(ard_num_dims=input_dim))\n",
    "                model_list.append(\n",
    "                    SingleTaskGP(\n",
    "                        train_x.to(torch.float64),\n",
    "                        train_y[:, obj_idx].unsqueeze(1).to(torch.float64),\n",
    "                        outcome_transform=Standardize(m=1),\n",
    "                        covar_module=covar_module\n",
    "                    ).to(device)\n",
    "                )\n",
    "            \n",
    "            model = ModelListGP(*model_list)\n",
    "            mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "            \n",
    "            try:\n",
    "                fit_gpytorch_mll(mll)\n",
    "            except Exception as e:\n",
    "                print(f'  Warning: Failed to fit GP: {e}')\n",
    "            \n",
    "            # Update training data for all models\n",
    "            for obj_idx in range(output_dim):\n",
    "                model.models[obj_idx].train_x = train_x\n",
    "                model.models[obj_idx].train_y = train_y[:, obj_idx].unsqueeze(1)\n",
    "            \n",
    "            # Optimize acquisition\n",
    "            custom_acq_function = CustomAcquisitionFunction(model, train_y, k_NN=k_NN, output_dim=output_dim)\n",
    "            acquisition_values = custom_acq_function(X_original.unsqueeze(1).to(torch.float32))\n",
    "            ids_sorted = acquisition_values.argsort(descending=True)\n",
    "            \n",
    "            for id_max in ids_sorted:\n",
    "                if id_max.item() not in ids_acquired:\n",
    "                    id_max_acquisition = id_max.item()\n",
    "                    break\n",
    "            \n",
    "            ids_acquired = np.concatenate((ids_acquired, [id_max_acquisition]))\n",
    "            train_x = X_original[ids_acquired].to(device)\n",
    "            train_y = Y_original[ids_acquired].to(device)\n",
    "            \n",
    "            coverage = reachability_uniformity(train_y, n_bins, mins, maxs, num_filled_grids)\n",
    "            coverage_list.append(coverage)\n",
    "            cost_list.append(cost_list[-1] + 1)\n",
    "        \n",
    "        cost_tensor.append(cost_list)\n",
    "        coverage_tensor.append(coverage_list)\n",
    "        print(f\"  Final coverage: {coverage:.4f}\")\n",
    "    \n",
    "    cost_tensor = torch.tensor(cost_tensor, dtype=torch.float32).cpu()\n",
    "    coverage_tensor = torch.tensor(coverage_tensor, dtype=torch.float32).cpu()\n",
    "    \n",
    "    torch.save(cost_tensor, 'RealData_cost_list_NS.pt')\n",
    "    torch.save(coverage_tensor, 'RealData_coverage_list_NS.pt')\n",
    "    print(\"\\nBEACON results saved.\")\n",
    "    \n",
    "    return cost_tensor, coverage_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067133f",
   "metadata": {},
   "source": [
    "### 4b. run-maxvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_maxvar(X_original, Y_original, N_init, BO_iter, n_bins, mins, maxs, \n",
    "               num_filled_grids, replicate, input_dim, output_dim, device):\n",
    "    \"\"\"Run MaxVar baseline.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Running MaxVar\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cost_tensor = []\n",
    "    coverage_tensor = []\n",
    "    \n",
    "    n_total_points = len(X_original)\n",
    "    \n",
    "    for seed in range(replicate):\n",
    "        print(f'\\n=== MaxVar Replicate {seed + 1}/{replicate} ===')\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        ids_acquired = np.random.choice(np.arange(n_total_points), size=N_init, replace=False)\n",
    "        train_x = X_original[ids_acquired].to(device)\n",
    "        train_y = Y_original[ids_acquired].to(device)\n",
    "        \n",
    "        coverage = reachability_uniformity(train_y, n_bins, mins, maxs, num_filled_grids)\n",
    "        coverage_list = [coverage]\n",
    "        cost_list = [0]\n",
    "        \n",
    "        for i in range(BO_iter):\n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"  Iteration {i + 1}/{BO_iter}, Coverage: {coverage:.4f}\")\n",
    "            \n",
    "            # Fit GPs for all output dimensions\n",
    "            model_list = []\n",
    "            for obj_idx in range(output_dim):\n",
    "                covar_module = ScaleKernel(RBFKernel(ard_num_dims=input_dim))\n",
    "                model_list.append(\n",
    "                    SingleTaskGP(\n",
    "                        train_x.to(torch.float64),\n",
    "                        train_y[:, obj_idx].unsqueeze(1).to(torch.float64),\n",
    "                        outcome_transform=Standardize(m=1),\n",
    "                        covar_module=covar_module\n",
    "                    ).to(device)\n",
    "                )\n",
    "            \n",
    "            model = ModelListGP(*model_list)\n",
    "            mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "            \n",
    "            try:\n",
    "                fit_gpytorch_mll(mll)\n",
    "            except Exception as e:\n",
    "                print(f'  Warning: Failed to fit GP: {e}')\n",
    "            \n",
    "            # MaxVar acquisition - sum variances across all outputs\n",
    "            acquisition_values = torch.zeros(len(X_original), device=device)\n",
    "            for obj_idx in range(output_dim):\n",
    "                acquisition_values += model.models[obj_idx].posterior(X_original).variance.flatten()\n",
    "            \n",
    "            ids_sorted = acquisition_values.argsort(descending=True)\n",
    "            \n",
    "            for id_max in ids_sorted:\n",
    "                if id_max.item() not in ids_acquired:\n",
    "                    id_max_acquisition = id_max.item()\n",
    "                    break\n",
    "            \n",
    "            ids_acquired = np.concatenate((ids_acquired, [id_max_acquisition]))\n",
    "            train_x = X_original[ids_acquired].to(device)\n",
    "            train_y = Y_original[ids_acquired].to(device)\n",
    "            \n",
    "            coverage = reachability_uniformity(train_y, n_bins, mins, maxs, num_filled_grids)\n",
    "            coverage_list.append(coverage)\n",
    "            cost_list.append(cost_list[-1] + 1)\n",
    "        \n",
    "        cost_tensor.append(cost_list)\n",
    "        coverage_tensor.append(coverage_list)\n",
    "        print(f\"  Final coverage: {coverage:.4f}\")\n",
    "    \n",
    "    cost_tensor = torch.tensor(cost_tensor, dtype=torch.float32).cpu()\n",
    "    coverage_tensor = torch.tensor(coverage_tensor, dtype=torch.float32).cpu()\n",
    "    \n",
    "    torch.save(cost_tensor, 'RealData_cost_list_MaxVar.pt')\n",
    "    torch.save(coverage_tensor, 'RealData_coverage_list_MaxVar.pt')\n",
    "    print(\"\\nMaxVar results saved.\")\n",
    "    \n",
    "    return cost_tensor, coverage_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b47d67",
   "metadata": {},
   "source": [
    "### 4c. run-Random-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4691e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_search(X_original, Y_original, N_init, BO_iter, n_bins, mins, maxs, \n",
    "                      num_filled_grids, replicate, input_dim, output_dim, device):\n",
    "    \"\"\"Run Random Search baseline.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Running Random Search\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cost_tensor = []\n",
    "    coverage_tensor = []\n",
    "    \n",
    "    n_total_points = len(X_original)\n",
    "    \n",
    "    for seed in range(replicate):\n",
    "        print(f'\\n=== Random Search Replicate {seed + 1}/{replicate} ===')\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        ids_acquired = np.random.choice(np.arange(n_total_points), size=N_init, replace=False)\n",
    "        train_x = X_original[ids_acquired].to(device)\n",
    "        train_y = Y_original[ids_acquired].to(device)\n",
    "        \n",
    "        coverage = reachability_uniformity(train_y, n_bins, mins, maxs, num_filled_grids)\n",
    "        coverage_list = [coverage]\n",
    "        cost_list = [0]\n",
    "        \n",
    "        for i in range(BO_iter):\n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"  Iteration {i + 1}/{BO_iter}, Coverage: {coverage:.4f}\")\n",
    "            \n",
    "            # Random acquisition\n",
    "            acquisition_values = torch.rand(n_total_points, device=device)\n",
    "            ids_sorted = acquisition_values.argsort(descending=True)\n",
    "            \n",
    "            for id_max in ids_sorted:\n",
    "                if id_max.item() not in ids_acquired:\n",
    "                    id_max_acquisition = id_max.item()\n",
    "                    break\n",
    "            \n",
    "            ids_acquired = np.concatenate((ids_acquired, [id_max_acquisition]))\n",
    "            train_x = X_original[ids_acquired].to(device)\n",
    "            train_y = Y_original[ids_acquired].to(device)\n",
    "            \n",
    "            coverage = reachability_uniformity(train_y, n_bins, mins, maxs, num_filled_grids)\n",
    "            coverage_list.append(coverage)\n",
    "            cost_list.append(cost_list[-1] + 1)\n",
    "        \n",
    "        cost_tensor.append(cost_list)\n",
    "        coverage_tensor.append(coverage_list)\n",
    "        print(f\"  Final coverage: {coverage:.4f}\")\n",
    "    \n",
    "    cost_tensor = torch.tensor(cost_tensor, dtype=torch.float32).cpu()\n",
    "    coverage_tensor = torch.tensor(coverage_tensor, dtype=torch.float32).cpu()\n",
    "    \n",
    "    torch.save(cost_tensor, 'RealData_cost_list_RS.pt')\n",
    "    torch.save(coverage_tensor, 'RealData_coverage_list_RS.pt')\n",
    "    print(\"\\nRandom Search results saved.\")\n",
    "    \n",
    "    return cost_tensor, coverage_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374776a",
   "metadata": {},
   "source": [
    "### 4d. run-novelty-search-feature-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ns_feature_space(X_original, Y_original, N_init, BO_iter, n_bins, mins, maxs, \n",
    "                         num_filled_grids, k_NN, replicate, input_dim, output_dim, device):\n",
    "    \"\"\"Run Novelty Search in Feature Space baseline.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Running NS-FS (Novelty Search in Feature Space)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    cost_tensor = []\n",
    "    coverage_tensor = []\n",
    "    \n",
    "    n_total_points = len(X_original)\n",
    "    \n",
    "    for seed in range(replicate):\n",
    "        print(f'\\n=== NS-FS Replicate {seed + 1}/{replicate} ===')\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        ids_acquired = np.random.choice(np.arange(n_total_points), size=N_init, replace=False)\n",
    "        train_x = X_original[ids_acquired].to(device)\n",
    "        train_y = Y_original[ids_acquired].to(device)\n",
    "        \n",
    "        coverage = reachability_uniformity(train_y, n_bins, mins, maxs, num_filled_grids)\n",
    "        coverage_list = [coverage]\n",
    "        cost_list = [0]\n",
    "        \n",
    "        for i in range(BO_iter):\n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"  Iteration {i + 1}/{BO_iter}, Coverage: {coverage:.4f}\")\n",
    "            \n",
    "            # NS in feature space\n",
    "            custom_acq_function = NoveltySearchFeatureSpace(train_x, k=k_NN)\n",
    "            acquisition_values = custom_acq_function(X_original)\n",
    "            ids_sorted = acquisition_values.argsort(descending=True)\n",
    "            \n",
    "            for id_max in ids_sorted:\n",
    "                if id_max.item() not in ids_acquired:\n",
    "                    id_max_acquisition = id_max.item()\n",
    "                    break\n",
    "            \n",
    "            ids_acquired = np.concatenate((ids_acquired, [id_max_acquisition]))\n",
    "            train_x = X_original[ids_acquired].to(device)\n",
    "            train_y = Y_original[ids_acquired].to(device)\n",
    "            \n",
    "            coverage = reachability_uniformity(train_y, n_bins, mins, maxs, num_filled_grids)\n",
    "            coverage_list.append(coverage)\n",
    "            cost_list.append(cost_list[-1] + 1)\n",
    "        \n",
    "        cost_tensor.append(cost_list)\n",
    "        coverage_tensor.append(coverage_list)\n",
    "        print(f\"  Final coverage: {coverage:.4f}\")\n",
    "    \n",
    "    cost_tensor = torch.tensor(cost_tensor, dtype=torch.float32).cpu()\n",
    "    coverage_tensor = torch.tensor(coverage_tensor, dtype=torch.float32).cpu()\n",
    "    \n",
    "    torch.save(cost_tensor, 'RealData_cost_list_NS_xspace.pt')\n",
    "    torch.save(coverage_tensor, 'RealData_coverage_list_NS_xspace.pt')\n",
    "    print(\"\\nNS-FS results saved.\")\n",
    "    \n",
    "    return cost_tensor, coverage_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42082224",
   "metadata": {},
   "source": [
    "## 5. Main function to run all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load data from disk\n",
    "print(\"\\nLOADING DATA FROM DISK\")\n",
    "print(\"=\"*80)\n",
    "data_filepath = 'your_data_file.pt'  # CHANGE THIS to your actual file path\n",
    "X_original, Y_original = load_data_from_disk(data_filepath, device=device)\n",
    "\n",
    "# Automatically detect dimensions\n",
    "input_dim = X_original.shape[1]\n",
    "output_dim = Y_original.shape[1]\n",
    "n_total_points = len(X_original)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"DATASET INFORMATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total points: {n_total_points}\")\n",
    "print(f\"Input dimensions: {input_dim}\")\n",
    "print(f\"Output dimensions: {output_dim}\")\n",
    "\n",
    "# Visualize loaded data\n",
    "visualize_loaded_data(X_original, Y_original)\n",
    "\n",
    "# Hyperparameters - adjust based on dataset size\n",
    "N_init = max(5, n_total_points // 10)  # 10% of data or at least 5 points\n",
    "max_BO_iter = n_total_points - N_init - 1  # Maximum possible iterations\n",
    "BO_iter = min(50, max_BO_iter)  # Use 50 or max available\n",
    "replicate = 5\n",
    "n_bins = 10\n",
    "k_NN = max(3, min(10, N_init // 2))  # Adjust k_NN based on init size\n",
    "\n",
    "print(f\"\\nHYPERPARAMETERS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Initial samples (N_init): {N_init}\")\n",
    "print(f\"BO iterations: {BO_iter}\")\n",
    "print(f\"Number of replicates: {replicate}\")\n",
    "print(f\"Grid bins: {n_bins}\")\n",
    "print(f\"k-NN: {k_NN}\")\n",
    "\n",
    "# Compute bounds for coverage metric\n",
    "mins = torch.min(Y_original, dim=0).values\n",
    "maxs = torch.max(Y_original, dim=0).values\n",
    "print(f\"\\nOUTPUT SPACE BOUNDS\")\n",
    "print(f\"{'='*80}\")\n",
    "for i in range(output_dim):\n",
    "    print(f\"Output {i}: [{mins[i]:.4f}, {maxs[i]:.4f}]\")\n",
    "\n",
    "# Calculate filled grids\n",
    "grid_sizes = (maxs - mins) / n_bins\n",
    "filled_grids = set()\n",
    "for point in Y_original:\n",
    "    grid_indices = torch.zeros(output_dim, dtype=torch.long, device=device)\n",
    "    for d in range(output_dim):\n",
    "        if point[d] >= maxs[d]:\n",
    "            grid_indices[d] = n_bins - 1\n",
    "        else:\n",
    "            grid_indices[d] = int((point[d] - mins[d]) / grid_sizes[d])\n",
    "    filled_grids.add(tuple(grid_indices.cpu().tolist()))\n",
    "\n",
    "num_filled_grids = len(filled_grids)\n",
    "max_possible_grids = n_bins ** output_dim\n",
    "print(f\"\\nGRID COVERAGE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total unique grids filled: {num_filled_grids} out of {max_possible_grids}\")\n",
    "print(f\"Initial coverage: {num_filled_grids/max_possible_grids*100:.2f}%\")\n",
    "\n",
    "# Run all methods\n",
    "cost_NS, coverage_NS = run_beacon(X_original, Y_original, N_init, BO_iter, n_bins, \n",
    "                                    mins, maxs, num_filled_grids, k_NN, replicate, \n",
    "                                    input_dim, output_dim, device)\n",
    "\n",
    "cost_MaxVar, coverage_MaxVar = run_maxvar(X_original, Y_original, N_init, BO_iter, \n",
    "                                            n_bins, mins, maxs, num_filled_grids, replicate, \n",
    "                                            input_dim, output_dim, device)\n",
    "\n",
    "cost_RS, coverage_RS = run_random_search(X_original, Y_original, N_init, BO_iter, \n",
    "                                            n_bins, mins, maxs, num_filled_grids, replicate, \n",
    "                                            input_dim, output_dim, device)\n",
    "\n",
    "cost_NS_xspace, coverage_NS_xspace = run_ns_feature_space(X_original, Y_original, N_init, \n",
    "                                                            BO_iter, n_bins, mins, maxs, \n",
    "                                                            num_filled_grids, k_NN, replicate, \n",
    "                                                            input_dim, output_dim, device)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING COMPARISON PLOTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute statistics\n",
    "coverage_NS_mean = torch.mean(coverage_NS, dim=0)\n",
    "coverage_NS_std = torch.std(coverage_NS, dim=0)\n",
    "cost_NS_mean = torch.mean(cost_NS, dim=0)\n",
    "\n",
    "coverage_MaxVar_mean = torch.mean(coverage_MaxVar, dim=0)\n",
    "coverage_MaxVar_std = torch.std(coverage_MaxVar, dim=0)\n",
    "cost_MaxVar_mean = torch.mean(cost_MaxVar, dim=0)\n",
    "\n",
    "coverage_RS_mean = torch.mean(coverage_RS, dim=0)\n",
    "coverage_RS_std = torch.std(coverage_RS, dim=0)\n",
    "cost_RS_mean = torch.mean(cost_RS, dim=0)\n",
    "\n",
    "coverage_NS_xspace_mean = torch.mean(coverage_NS_xspace, dim=0)\n",
    "coverage_NS_xspace_std = torch.std(coverage_NS_xspace, dim=0)\n",
    "cost_NS_xspace_mean = torch.mean(cost_NS_xspace, dim=0)\n",
    "\n",
    "# Plot\n",
    "text_size = 24\n",
    "marker_size = 18\n",
    "weight = 'bold'\n",
    "alpha = 0.3\n",
    "linewidth = 4\n",
    "marker_interval = max(1, BO_iter // 10)  # Adaptive marker interval\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.plot(cost_NS_mean[::marker_interval], coverage_NS_mean[::marker_interval], \n",
    "            label='BEACON', marker='X', markersize=marker_size, linewidth=linewidth)\n",
    "plt.plot(cost_MaxVar_mean[::marker_interval], coverage_MaxVar_mean[::marker_interval], \n",
    "            label='MaxVar', marker='^', markersize=marker_size, linewidth=linewidth)\n",
    "plt.plot(cost_NS_xspace_mean[::marker_interval], coverage_NS_xspace_mean[::marker_interval], \n",
    "            label='NS-FS', marker='p', markersize=marker_size, color='mediumpurple', linewidth=linewidth)\n",
    "plt.plot(cost_RS_mean[::marker_interval], coverage_RS_mean[::marker_interval], \n",
    "            label='RS', marker='v', markersize=marker_size, color='hotpink', linewidth=linewidth)\n",
    "\n",
    "plt.fill_between(cost_NS_mean, coverage_NS_mean - coverage_NS_std, \n",
    "                    coverage_NS_mean + coverage_NS_std, alpha=alpha)\n",
    "plt.fill_between(cost_MaxVar_mean, coverage_MaxVar_mean - coverage_MaxVar_std, \n",
    "                    coverage_MaxVar_mean + coverage_MaxVar_std, alpha=alpha)\n",
    "plt.fill_between(cost_NS_xspace_mean, coverage_NS_xspace_mean - coverage_NS_xspace_std, \n",
    "                    coverage_NS_xspace_mean + coverage_NS_xspace_std, alpha=alpha, color='mediumpurple')\n",
    "plt.fill_between(cost_RS_mean, coverage_RS_mean - coverage_RS_std, \n",
    "                    coverage_RS_mean + coverage_RS_std, alpha=alpha, color='hotpink')\n",
    "\n",
    "plt.xlabel('Number of evaluations', fontsize=text_size, fontweight=weight)\n",
    "plt.ylabel('Reachability', fontsize=text_size, fontweight=weight)\n",
    "plt.legend(prop={'weight': 'bold', 'size': text_size}, loc='best')\n",
    "plt.title(f'Real Data ({n_total_points} points, input_dim={input_dim}, output_dim={output_dim})', \n",
    "            fontsize=text_size, fontweight=weight)\n",
    "\n",
    "plt.tick_params(axis='both', which='both', width=2)\n",
    "ax = plt.gca()\n",
    "for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "    label.set_fontsize(text_size)\n",
    "    label.set_fontweight('bold')\n",
    "\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(2)\n",
    "\n",
    "plt.grid(alpha=0.5, linewidth=2.0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_all_methods_realdata.png\", dpi=150, bbox_inches='tight')\n",
    "\n",
    "# Print GPU memory usage if applicable\n",
    "if device.type == 'cuda':\n",
    "    print(f\"\\nFinal GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB\")\n",
    "    print(f\"Peak GPU Memory Allocated: {torch.cuda.max_memory_allocated(0)/1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qBO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
